{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus \n",
    "## Entrevistas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convierte archivos de entrevistas en formato Excel (.xlsx) a CSV (.csv), facilitando su procesamiento posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido a CSV con ,: ./data/interview/Leonidas Iza.csv\n",
      "Archivo convertido a CSV con ,: ./data/interview/HenryKronfle.csv\n",
      "Archivo convertido a CSV con ,: ./data/interview/Luis Tilleria.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Archivos Excel de entrada\n",
    "archivos_excel = [\n",
    "    './data/interview/Leonidas Iza.xlsx',\n",
    "    './data/interview/HenryKronfle.xlsx',\n",
    "    './data/interview/Luis Tilleria.xlsx'\n",
    "]\n",
    "\n",
    "# Convertir cada archivo Excel a CSV con ,\n",
    "for archivo_excel in archivos_excel:\n",
    "    # Leer el archivo Excel\n",
    "    df = pd.read_excel(archivo_excel)\n",
    "    \n",
    "    # Definir el archivo CSV de salida\n",
    "    archivo_csv = archivo_excel.replace('.xlsx', '.csv')  # Reemplaza la extensi√≥n .xlsx por .csv\n",
    "    \n",
    "    # Guardar como archivo CSV con ,\n",
    "    df.to_csv(archivo_csv, index=False, sep=',')  # Usamos tabulaci√≥n como separador\n",
    "    \n",
    "    print(f\"Archivo convertido a CSV con ,: {archivo_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrae temas clave y genera res√∫menes de entrevistas utilizando Ollama, guardando la informaci√≥n en CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo procesado: ./data/interview/Leonidas_Iza_pross.csv\n",
      "‚úÖ Archivo procesado: ./data/interview/Henry_Kronfle_pross.csv\n",
      "‚úÖ Archivo procesado: ./data/interview/Luis_Tilleria_pross.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ollama  # Aseg√∫rate de tener instalada la librer√≠a con: pip install ollama\n",
    "\n",
    "# Lista de temas relevantes\n",
    "temas_relevantes = { \n",
    "    \"econom√≠a\", \"educaci√≥n\", \"salud\", \"seguridad\", \"empleo\",\n",
    "    \"infraestructura\", \"corrupci√≥n\", \"tecnolog√≠a\", \"ambiente\",\n",
    "    \"justicia\", \"transporte\", \"pol√≠tica\", \"desarrollo\", \"energ√≠a\",\n",
    "    \"derechos humanos\", \"igualdad\", \"innovaci√≥n\", \"turismo\",\n",
    "    \"agricultura\", \"cultura\", \"deporte\", \"finanzas\", \"inversi√≥n\",\n",
    "    \"vivienda\", \"servicios p√∫blicos\", \"ciencia\", \"medio ambiente\",\n",
    "    \"gobierno\", \"industria\", \"exportaciones\", \"importaciones\",\n",
    "    \"educaci√≥n superior\", \"sanidad\", \"movilidad\", \"inteligencia artificial\",\n",
    "    \"seguridad ciudadana\", \"crimen organizado\", \"democracia\", \"pobreza\",\n",
    "    \"sostenibilidad\", \"digitalizaci√≥n\", \"gesti√≥n p√∫blica\", \"comercio\",\n",
    "    \"cambio clim√°tico\", \"energ√≠as renovables\", \"transparencia\", \"ciberseguridad\",\n",
    "    \"salud p√∫blica\", \"gobernanza\", \"justicia social\", \"igualdad de g√©nero\",\n",
    "    \"emprendimiento\", \"industria 4.0\", \"desarrollo sostenible\", \"desastres naturales\",\n",
    "    \"reforestaci√≥n\", \"movilidad urbana\", \"biodiversidad\", \"educaci√≥n financiera\",\n",
    "    \"trabajo remoto\", \"accesibilidad\", \"industria alimentaria\", \"industria tecnol√≥gica\",\n",
    "    \"educaci√≥n digital\", \"cultura digital\", \"sociedad del conocimiento\", \n",
    "    \"banca digital\", \"teletrabajo\", \"inteligencia colectiva\", \"biotecnolog√≠a\",\n",
    "    \"blockchain\", \"fintech\", \"medicina personalizada\", \"econom√≠a circular\",\n",
    "    \"ciudades inteligentes\", \"protecci√≥n de datos\", \"energ√≠a solar\", \"transporte el√©ctrico\",\n",
    "    \"robotizaci√≥n\", \"computaci√≥n cu√°ntica\", \"espacio exterior\", \"protecci√≥n ambiental\",\n",
    "    \"seguridad en la nube\", \"movilidad el√©ctrica\", \"alimentos org√°nicos\", \"tecnolog√≠a educativa\",\n",
    "    \"agtech\", \"neurociencia\", \"edtech\", \"deep learning\", \"big data\", \"sistemas aut√≥nomos\",\n",
    "    \"tecnolog√≠a espacial\", \"cambio de paradigma\", \"smart grids\", \"ciudades sostenibles\", \n",
    "    \"ecoeficiencia\", \"energ√≠a e√≥lica\", \"tecnolog√≠as disruptivas\", \"energ√≠a geot√©rmica\",\n",
    "    \"nanotecnolog√≠a\", \"microbioma\", \"bioeconom√≠a\", \"ecoturismo\", \"industrias creativas\",\n",
    "    \"gobernanza digital\", \"energ√≠a limpia\", \"criptomonedas\", \"miner√≠a digital\", \"ciencias marinas\",\n",
    "    \"nanomateriales\", \"inteligencia emocional\", \"finanzas sostenibles\", \"educaci√≥n en l√≠nea\",\n",
    "    \"bio\", \"ecoinnovaci√≥n\", \"simulaci√≥n computacional\", \"agricultura urbana\", \"cultivos inteligentes\",\"IESS\"\n",
    "}\n",
    "\n",
    "# üìå Funci√≥n para extraer los temas tratados\n",
    "def extraer_temas(texto):\n",
    "    # Aseguramos que el texto no sea vac√≠o o nulo\n",
    "    if not texto or not isinstance(texto, str):\n",
    "        return \"Otros\"\n",
    "    \n",
    "    # Convertimos el texto a min√∫sculas y buscamos los temas\n",
    "    texto = texto.lower()\n",
    "    temas_detectados = [tema for tema in temas_relevantes if tema.lower() in texto]\n",
    "    \n",
    "    # Si encontramos temas, los unimos en una cadena separada por comas\n",
    "    return \", \".join(temas_detectados) if temas_detectados else \"Otros\"\n",
    "\n",
    "# üìå Funci√≥n para generar res√∫menes con Ollama\n",
    "def generar_resumen_ollama(texto):\n",
    "    prompt = f\"Resume el siguiente texto en un parrafo:\\n\\n{texto}\"\n",
    "\n",
    "    try:\n",
    "        respuesta = ollama.chat(\n",
    "            model=\"llama3.2:latest\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        resumen = respuesta.get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "        # Eliminar saltos de l√≠nea y espacios extras\n",
    "        resumen = ' '.join(resumen.split())\n",
    "        return resumen if resumen else \"No se pudo generar el resumen\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error con Ollama: {e}\")\n",
    "        return \"Resumen no disponible\"\n",
    "\n",
    "def procesar_csv(archivo_entrada, archivo_salida):\n",
    "    df = pd.read_csv(archivo_entrada,sep=\",\")\n",
    "\n",
    "    # Verifica si la columna \"entrevista\" existe en el archivo CSV\n",
    "    if \"entrevista\" not in df.columns:\n",
    "        print(\"‚ùå La columna 'entrevista' no se encuentra en el archivo.\")\n",
    "        return\n",
    "\n",
    "    # Extraer temas tratados\n",
    "    df[\"Temas_Tratados\"] = df[\"entrevista\"].astype(str).apply(extraer_temas)\n",
    "\n",
    "    # Generar res√∫menes con Ollama\n",
    "    df[\"descripcion\"] = df[\"entrevista\"].astype(str).apply(generar_resumen_ollama)\n",
    "\n",
    "    # Guardar el archivo procesado\n",
    "    df.to_csv(archivo_salida, index=False, encoding=\"utf-8-sig\", sep=\",\")\n",
    "    print(f\"‚úÖ Archivo procesado: {archivo_salida}\")\n",
    "\n",
    "# üî• Ejecutar para m√∫ltiples archivos CSV\n",
    "procesar_csv(\"./data/interview/Leonidas Iza.csv\", \"./data/interview/Leonidas_Iza_pross.csv\")\n",
    "procesar_csv(\"./data/interview/HenryKronfle.csv\", \"./data/interview/Henry_Kronfle_pross.csv\")\n",
    "procesar_csv(\"./data/interview/Luis Tilleria.csv\", \"./data/interview/Luis_Tilleria_pross.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estandariza y limpia los archivos CSV de entrevistas, corrigiendo nombres de columnas, rellenando valores faltantes, asignando ID secuenciales y reordenando las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Andrea_Gonzalez.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": carlos_rabascall.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Daniel_Noboa.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": HenryKronfle.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Henry_Kronfle_pross.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": JimmyJairala.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": JorgeEscala.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Leonidas Iza.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Leonidas_Iza_pross.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Luis Tilleria.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Luis_Tilleria_pross.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": pedro_granja.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": victor_araus.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Wilson_Gomez.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Carpeta que contiene los archivos CSV\n",
    "directorio_csv = './data/interview'  # Cambia por la ruta de tu carpeta con archivos CSV\n",
    "\n",
    "# Definir las columnas en el orden que quieres\n",
    "columnas_ordenadas = ['ID', 'Candidato', 'Temas_Tratados', 'Descripcion', 'Entrevista']\n",
    "\n",
    "# Funci√≥n para encontrar el nombre m√°s similar a una lista de nombres\n",
    "def encontrar_columna_similar(nombre_columna, lista_columnas):\n",
    "    import difflib\n",
    "    # Encuentra la coincidencia m√°s cercana\n",
    "    coincidencias = difflib.get_close_matches(nombre_columna, lista_columnas, n=1, cutoff=0.8)\n",
    "    return coincidencias[0] if coincidencias else None\n",
    "\n",
    "# Funci√≥n para depurar un DataFrame\n",
    "def depurar_dataframe(df):\n",
    "    # Rellenar valores NaN con un valor vac√≠o o 0 dependiendo del tipo de la columna\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':  # Si es columna de tipo texto\n",
    "            df[col].fillna('', inplace=True)\n",
    "        else:  # Si es columna num√©rica\n",
    "            df[col].fillna(0, inplace=True)\n",
    "    \n",
    "    # Verificar si todas las columnas esperadas est√°n presentes\n",
    "    for col in columnas_ordenadas:\n",
    "        if col not in df.columns:\n",
    "            df[col] = ''  # Si falta alguna columna, agregarla como vac√≠a\n",
    "    \n",
    "    # Asegurarse de que el DataFrame no est√© vac√≠o\n",
    "    if df.empty:\n",
    "        print(\"El DataFrame est√° vac√≠o, no se guardar√°.\")\n",
    "        return None\n",
    "    \n",
    "    # Asignar numeraci√≥n secuencial a la columna 'ID'\n",
    "    df['ID'] = range(1, len(df) + 1)\n",
    "    \n",
    "    # Reordenar las columnas del DataFrame\n",
    "    df = df[columnas_ordenadas]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta\n",
    "for archivo in os.listdir(directorio_csv):\n",
    "    if archivo.endswith('.csv'):\n",
    "        archivo_csv = os.path.join(directorio_csv, archivo)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(archivo_csv, sep=',', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer el archivo {archivo}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Renombrar las columnas si hay coincidencias\n",
    "        columnas_actuales = df.columns.tolist()\n",
    "        columnas_renombradas = {}\n",
    "        \n",
    "        for col in columnas_actuales:\n",
    "            columna_similar = encontrar_columna_similar(col, columnas_ordenadas)\n",
    "            if columna_similar:\n",
    "                columnas_renombradas[col] = columna_similar\n",
    "        \n",
    "        # Renombrar las columnas en el DataFrame\n",
    "        df.rename(columns=columnas_renombradas, inplace=True)\n",
    "        \n",
    "        # Depurar el DataFrame\n",
    "        df_limpio = depurar_dataframe(df)\n",
    "        \n",
    "        if df_limpio is not None:\n",
    "            # Guardar el archivo CSV con el nuevo orden de columnas y delimitador \";\"\n",
    "            df_limpio.to_csv(archivo_csv, index=False, sep=',')\n",
    "            print(f'Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": {archivo}')\n",
    "        else:\n",
    "            print(f'El archivo {archivo} no se guard√≥ debido a que est√° vac√≠o o con errores.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unir entrevistas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unifica m√∫ltiples archivos CSV de entrevistas, corrige nombres de columnas, normaliza valores, asigna IDs secuenciales y guarda el resultado en un solo archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo combinado guardado como: ./data/interview/archivo_combinado.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Carpeta que contiene los archivos CSV\n",
    "directorio_csv = './data/interview/'  # Cambia por la ruta de tu carpeta con archivos CSV\n",
    "\n",
    "# Definir las columnas en el orden que quieres\n",
    "columnas_ordenadas = ['ID', 'Candidato', 'Temas_Tratados', 'Descripcion', 'Entrevista']\n",
    "\n",
    "# Funci√≥n para encontrar el nombre m√°s similar a una lista de nombres\n",
    "def encontrar_columna_similar(nombre_columna, lista_columnas):\n",
    "    import difflib\n",
    "    # Encuentra la coincidencia m√°s cercana\n",
    "    coincidencias = difflib.get_close_matches(nombre_columna, lista_columnas, n=1, cutoff=0.8)\n",
    "    return coincidencias[0] if coincidencias else None\n",
    "\n",
    "# Funci√≥n para depurar un DataFrame\n",
    "def depurar_dataframe(df):\n",
    "    # Rellenar valores NaN con un valor vac√≠o o 0 dependiendo del tipo de la columna\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':  # Si es columna de tipo texto\n",
    "            df[col].fillna('', inplace=True)\n",
    "        else:  # Si es columna num√©rica\n",
    "            df[col].fillna(0, inplace=True)\n",
    "    \n",
    "    # Verificar si todas las columnas esperadas est√°n presentes\n",
    "    for col in columnas_ordenadas:\n",
    "        if col not in df.columns:\n",
    "            df[col] = ''  # Si falta alguna columna, agregarla como vac√≠a\n",
    "    \n",
    "    # Asegurarse de que el DataFrame no est√© vac√≠o\n",
    "    if df.empty:\n",
    "        print(\"El DataFrame est√° vac√≠o, no se guardar√°.\")\n",
    "        return None\n",
    "    \n",
    "    # Reordenar las columnas del DataFrame\n",
    "    df = df[columnas_ordenadas]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Lista para almacenar todos los DataFrames\n",
    "df_final = []\n",
    "id_contador = 1  # Contador para los IDs secuenciales\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta\n",
    "for archivo in os.listdir(directorio_csv):\n",
    "    if archivo.endswith('.csv'):\n",
    "        archivo_csv = os.path.join(directorio_csv, archivo)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(archivo_csv, sep=',', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer el archivo {archivo}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Renombrar las columnas si hay coincidencias\n",
    "        columnas_actuales = df.columns.tolist()\n",
    "        columnas_renombradas = {}\n",
    "        \n",
    "        for col in columnas_actuales:\n",
    "            columna_similar = encontrar_columna_similar(col, columnas_ordenadas)\n",
    "            if columna_similar:\n",
    "                columnas_renombradas[col] = columna_similar\n",
    "        \n",
    "        # Renombrar las columnas en el DataFrame\n",
    "        df.rename(columns=columnas_renombradas, inplace=True)\n",
    "        \n",
    "        # Depurar el DataFrame\n",
    "        df_limpio = depurar_dataframe(df)\n",
    "        \n",
    "        if df_limpio is not None:\n",
    "            # Asignar los IDs secuenciales antes de agregar el DataFrame al archivo final\n",
    "            df_limpio['ID'] = range(id_contador, id_contador + len(df_limpio))\n",
    "            id_contador += len(df_limpio)  # Incrementar el contador para el siguiente archivo\n",
    "            \n",
    "            # Agregar el DataFrame limpio a la lista\n",
    "            df_final.append(df_limpio)\n",
    "        else:\n",
    "            print(f'El archivo {archivo} no se guard√≥ debido a que est√° vac√≠o o con errores.')\n",
    "\n",
    "# Combinar todos los DataFrames en uno solo\n",
    "df_combinado = pd.concat(df_final, ignore_index=True)\n",
    "\n",
    "# Guardar el archivo combinado en un nuevo CSV\n",
    "archivo_combinado = './data/interview/archivo_combinado.csv'\n",
    "df_combinado.to_csv(archivo_combinado, index=False, sep=',')\n",
    "\n",
    "print(f'Archivo combinado guardado como: {archivo_combinado}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpia, tokeniza y segmenta las entrevistas en oraciones individuales, asignando IDs √∫nicos y guardando el resultado en un CSV expandido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo expandido guardado como: ./data/interview/archivo_combinado_expandido.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Funci√≥n para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''  # Si no es un string, devolver vac√≠o\n",
    "    \n",
    "    # Eliminar vi√±etas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    \n",
    "    # Eliminar enumeraciones (n√∫meros seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la l√≠nea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "    \n",
    "    # Reemplazar m√∫ltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Funci√≥n para dividir el texto en oraciones por ID\n",
    "def dividir_oraciones_por_id(text, text_id):\n",
    "    delimitadores = '.'\n",
    "    oraciones = []\n",
    "    oracion_actual = \"\"\n",
    "    \n",
    "    for char in text:\n",
    "        oracion_actual += char\n",
    "        if char in delimitadores:\n",
    "            oraciones.append(oracion_actual.strip())\n",
    "            oracion_actual = \"\"\n",
    "    \n",
    "    if oracion_actual:  # Si hay algo restante\n",
    "        oraciones.append(oracion_actual.strip())\n",
    "    \n",
    "    # Crear una lista de tuplas con ID y oraciones\n",
    "    return [(text_id, i, oracion) for i, oracion in enumerate(oraciones, start=1)]\n",
    "\n",
    "# Cargar el archivo combinado\n",
    "archivo_combinado = './data/interview/archivo_combinado.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(archivo_combinado)\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: No se encontr√≥ el archivo {archivo_combinado}\")\n",
    "    exit()\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"‚ùå Error: El archivo {archivo_combinado} est√° vac√≠o.\")\n",
    "    exit()\n",
    "\n",
    "# Verificar si la columna 'ID' ya existe\n",
    "if 'ID' in df.columns:\n",
    "    df = df.sort_values(by='ID').reset_index(drop=True)  # Ordenar por ID y resetear √≠ndices\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Advertencia: La columna 'ID' no existe. Se generar√° numeraci√≥n autom√°tica.\")\n",
    "    df['ID'] = range(1, len(df) + 1)\n",
    "\n",
    "# Crear lista para almacenar las nuevas filas con oraciones separadas\n",
    "nuevas_filas = []\n",
    "\n",
    "# Procesar cada fila del dataframe\n",
    "for _, row in df.iterrows():\n",
    "    text_id = row['ID']\n",
    "    texto_limpio = clean_content(row.get('Entrevista', ''))  # Limpiar el texto antes de dividirlo\n",
    "    oraciones = dividir_oraciones_por_id(texto_limpio, text_id)  # Extraer oraciones con ID\n",
    "    \n",
    "    for id_original, id_oracion, oracion in oraciones:\n",
    "        nueva_fila = row.drop(labels=['Entrevista'], errors='ignore').to_dict()  # Evitar KeyError\n",
    "        nueva_fila['ID_Oracion'] = f\"{id_original}-{id_oracion}\"  # ID √∫nico para cada oraci√≥n\n",
    "        nueva_fila['Oracion_Entrevista'] = oracion\n",
    "        nuevas_filas.append(nueva_fila)\n",
    "\n",
    "# Convertir la lista de nuevas filas en un DataFrame\n",
    "df_expandido = pd.DataFrame(nuevas_filas)\n",
    "\n",
    "# Reajustar la columna ID para que sea secuencial\n",
    "df_expandido['ID'] = range(1, len(df_expandido) + 1)\n",
    "\n",
    "# Guardar el archivo actualizado con oraciones separadas\n",
    "archivo_expandido = './data/interview/archivo_combinado_expandido.csv'\n",
    "df_expandido.to_csv(archivo_expandido, index=False, sep=',')\n",
    "\n",
    "print(f'‚úÖ Archivo expandido guardado como: {archivo_expandido}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fusiona oraciones procesadas con informaci√≥n pol√≠tica por partido, eliminando columnas innecesarias y guardando el resultado en un CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros valores de 'Partido' en oracion_df:\n",
      "0    REVOLUCI√ìN CIUDADANA - RETO\n",
      "1    REVOLUCI√ìN CIUDADANA - RETO\n",
      "2    REVOLUCI√ìN CIUDADANA - RETO\n",
      "3    REVOLUCI√ìN CIUDADANA - RETO\n",
      "4    REVOLUCI√ìN CIUDADANA - RETO\n",
      "Name: Partido, dtype: object\n",
      "Primeros valores de 'Partido' en completo_df:\n",
      "0    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "1    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "2    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "3    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "4    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "Name: Partido, dtype: object\n",
      "Primeras filas del archivo fusionado:\n",
      "    id  oracion_id                                   oracion_original  \\\n",
      "0  AG1           1  Andrea Gonz√°lez Nader, candidata presidencial ...   \n",
      "1  AG1           2  Propone una reducci√≥n del aparato estatal medi...   \n",
      "2  AG1           3  En materia de seguridad, destaca la necesidad ...   \n",
      "3  AG1           4  Adem√°s, promueve una reforma constitucional si...   \n",
      "4  AG1           5  En el √°mbito de salud, propone un enfoque prev...   \n",
      "\n",
      "                                      oracion_limpia  \\\n",
      "0  andrea gonzalez nader candidata presidencial p...   \n",
      "1  propone una reduccion del aparato estatal medi...   \n",
      "2  en materia de seguridad destaca la necesidad d...   \n",
      "3  ademas promueve una reforma constitucional sin...   \n",
      "4  en el ambito de salud propone un enfoque preve...   \n",
      "\n",
      "                                oracion_sinStopWords       presidente  \\\n",
      "0  andrea gonzalez nader candidata presidencial s...  ANDREA GONZALEZ   \n",
      "1  propone reduccion aparato estatal mediante eli...  ANDREA GONZALEZ   \n",
      "2  materia seguridad destaca necesidad retomar co...  ANDREA GONZALEZ   \n",
      "3  ademas promueve reforma constitucional recurri...  ANDREA GONZALEZ   \n",
      "4  ambito salud propone enfoque preventivo lugar ...  ANDREA GONZALEZ   \n",
      "\n",
      "  vicepresidente  lista                                  Partido Oracion  \\\n",
      "0   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO     NaN   \n",
      "1   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO     NaN   \n",
      "2   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO     NaN   \n",
      "3   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO     NaN   \n",
      "4   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO     NaN   \n",
      "\n",
      "  Temas Clave  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3         NaN  \n",
      "4         NaN  \n",
      "Proceso completado. Archivo guardado como 'partidopol.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los archivos\n",
    "oracion_df = pd.read_csv('oraciones_procesadas_completo.csv', sep=\";\")\n",
    "completo_df = pd.read_csv('./data/interview/elecciones_oraciones.csv', sep=\",\")\n",
    "\n",
    "# Verificar los primeros valores de la columna 'Partido' para ambos DataFrames\n",
    "print(\"Primeros valores de 'Partido' en oracion_df:\")\n",
    "print(oracion_df['Partido'].head())\n",
    "print(\"Primeros valores de 'Partido' en completo_df:\")\n",
    "print(completo_df['Partido'].head())\n",
    "\n",
    "# Asegurarse de que las columnas 'Partido' sean del mismo tipo (string)\n",
    "oracion_df['Partido'] = oracion_df['Partido'].astype(str)\n",
    "completo_df['Partido'] = completo_df['Partido'].astype(str)\n",
    "\n",
    "# Excluir las columnas no deseadas de 'Oracion.csv'\n",
    "columnas_excluir = ['ID', 'Oracion_ID', 'CandidatoPresidente', 'CandidatoVicePresidente', 'ListaPolitica']\n",
    "oracion_df_limpio = oracion_df.drop(columns=columnas_excluir)\n",
    "\n",
    "# Fusionar por la columna 'Partido' y agregar las columnas 'Oracion' y 'Temas Clave'\n",
    "completo_df = completo_df.merge(oracion_df_limpio[['Partido', 'Oracion', 'Temas Clave']], on='Partido', how='left')\n",
    "\n",
    "# Verificar la fusi√≥n\n",
    "print(\"Primeras filas del archivo fusionado:\")\n",
    "print(completo_df.head())\n",
    "\n",
    "# Guardar el archivo actualizado\n",
    "completo_df.to_csv(\"partidopol.csv\", index=False)\n",
    "\n",
    "print(\"Proceso completado. Archivo guardado como 'partidopol.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga los archivos CSV y verifica los primeros valores de la columna Partido en ambos DataFrames para validar su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    REVOLUCI√ìN CIUDADANA - RETO\n",
      "1    REVOLUCI√ìN CIUDADANA - RETO\n",
      "2    REVOLUCI√ìN CIUDADANA - RETO\n",
      "3    REVOLUCI√ìN CIUDADANA - RETO\n",
      "4    REVOLUCI√ìN CIUDADANA - RETO\n",
      "Name: Partido, dtype: object\n",
      "0    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "1    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "2    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "3    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "4    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "Name: Partido, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Cargar los archivos\n",
    "oracion_df = pd.read_csv('oraciones_procesadas_completo.csv', sep=\";\")\n",
    "completo_df = pd.read_csv('./data/interview/elecciones_oraciones.csv', sep=\",\")\n",
    "print(oracion_df['Partido'].head())\n",
    "print(completo_df['Partido'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# columna id, partido politico,content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registra los nombres de los partidos pol√≠ticos desde los archivos PDF de planes de trabajo, asignando IDs √∫nicos y almacenando la informaci√≥n en un CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: candidatos.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde est√°n los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"candidatos.csv\"\n",
    "\n",
    "# Lista de diccionarios espec√≠ficos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCI√ìN CIUDADANA - RETO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA M√ÅS ACCI√ìN, SUMA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCR√ÅTICA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCR√ÅTICO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCI√ìN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRI√ìTICA  21 DE ENERO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA S√ç _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\"},\n",
    "]\n",
    "\n",
    "# Funci√≥n para obtener el √∫ltimo ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios espec√≠ficos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Agregar los datos a la lista\n",
    "        data.append([file_id, processed_name])\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de los datos nuevos\n",
    "df_new = pd.DataFrame(data, columns=['ID', 'Nombre'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_new\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrae, limpia y segmenta el contenido de archivos PDF de planes de gobierno en oraciones individuales, asignando IDs √∫nicos y almacen√°ndolos en un CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: oraciones.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde est√°n los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"oraciones.csv\"\n",
    "\n",
    "# Lista de diccionarios espec√≠ficos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCI√ìN CIUDADANA - RETO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 8},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA M√ÅS ACCI√ìN, SUMA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 7},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCR√ÅTICA _Plan de trabajo_.pdf\",\"exclude_pages_start\": 5},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCR√ÅTICO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCI√ìN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRI√ìTICA  21 DE ENERO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA S√ç _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\", \"exclude_pages_start\": 1}\n",
    "]\n",
    "\n",
    "# Funci√≥n para obtener el √∫ltimo ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "# Funci√≥n para extraer texto del PDF excluyendo las primeras y √∫ltimas p√°ginas\n",
    "def extract_text_excluding_pages(pdf_path, exclude_pages_start, exclude_pages_end=1):\n",
    "    extracted_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i in range(exclude_pages_start, len(pdf.pages) - exclude_pages_end):\n",
    "            page_text = pdf.pages[i].extract_text()\n",
    "            if page_text:\n",
    "                extracted_text += page_text + \"\\n\"\n",
    "    return extracted_text.strip()\n",
    "\n",
    "# Funci√≥n para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    # Eliminar vi√±etas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    # Eliminar enumeraciones (n√∫meros seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la l√≠nea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "    \n",
    "    # Reemplazar m√∫ltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Funci√≥n para dividir el texto en oraciones\n",
    "def dividir_oraciones_por_id(text, text_id):\n",
    "    delimitadores = '.'\n",
    "    oraciones = []\n",
    "    oracion_actual = \"\"\n",
    "    for char in text:\n",
    "        oracion_actual += char\n",
    "        if char in delimitadores:\n",
    "            oraciones.append(oracion_actual.strip())\n",
    "            oracion_actual = \"\"\n",
    "    if oracion_actual:  # Si hay algo restante\n",
    "        oraciones.append(oracion_actual.strip())\n",
    "    \n",
    "    # Crear una lista de tuplas con id y oraciones\n",
    "    return [(text_id, i, oracion) for i, oracion in enumerate(oraciones, start=1)]\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios espec√≠ficos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "    exclude_pages_start = file_param[\"exclude_pages_start\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Extraer el contenido del PDF\n",
    "        content = extract_text_excluding_pages(pdf_path, exclude_pages_start=exclude_pages_start)\n",
    "\n",
    "        # Limpiar el contenido extra√≠do\n",
    "        cleaned_content = clean_content(content)\n",
    "\n",
    "        # Dividir el contenido en oraciones\n",
    "        oraciones = dividir_oraciones_por_id(cleaned_content, file_id)\n",
    "\n",
    "        # Agregar las oraciones a la lista de datos\n",
    "        data.extend(oraciones)\n",
    "\n",
    "        # Incrementar el ID\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de las oraciones\n",
    "df_oraciones = pd.DataFrame(data, columns=['ID', 'Oracion_ID', 'Oracion'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_oraciones], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_oraciones\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convierte PDFs escaneados en im√°genes, extrae texto con Tesseract OCR, lo segmenta en oraciones, asigna IDs √∫nicos y guarda los resultados en un CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf procesado (ID 11)\n",
      "MOVIMIENTO DEMOCRACIA S√ç _Plan de trabajo_.pdf procesado (ID 13)\n",
      "PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf procesado (ID 15)\n",
      "\n",
      "Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar el recurso necesario para tokenizar oraciones\n",
    "nltk.download('punkt')\n",
    "\n",
    "csv.field_size_limit(1000000)\n",
    "\n",
    "# Configuraci√≥n global\n",
    "pdf_directory = \"./data/\"\n",
    "csv_file = \"oraciones.csv\"\n",
    "columns = ['ID', 'Nombre', 'Contenido']\n",
    "\n",
    "# Configurar Tesseract para Fedora\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "def procesar_pdf(ruta_pdf, id_asignado, nombre_doc):\n",
    "    try:\n",
    "        # Convertir PDF a im√°genes\n",
    "        images = convert_from_path(ruta_pdf, dpi=300)\n",
    "        \n",
    "        # Extraer y limpiar texto\n",
    "        contenido = \" \".join(\n",
    "            [pytesseract.image_to_string(img, lang='spa').strip().replace('\\n', ' ') \n",
    "             for img in images]\n",
    "        )\n",
    "        \n",
    "        # Tokenizar el texto en oraciones\n",
    "        oraciones = sent_tokenize(contenido, language='spanish')\n",
    "        \n",
    "        # Asignar ID √∫nico a cada oraci√≥n\n",
    "        oraciones_ids = []\n",
    "        oraciones_texto = []\n",
    "        \n",
    "        for i, oracion in enumerate(oraciones):\n",
    "            oraciones_ids.append(f\"{id_asignado}_{i}\")  # ID √∫nico para cada oraci√≥n\n",
    "            oraciones_texto.append(oracion)  # Texto de la oraci√≥n\n",
    "        \n",
    "        # Crear DataFrame con solo los campos requeridos\n",
    "        data = {\n",
    "            'ID': [id_asignado] * len(oraciones),\n",
    "            'Oracion_ID': oraciones_ids,\n",
    "            'Oracion': oraciones_texto\n",
    "        }\n",
    "        df_oraciones = pd.DataFrame(data, columns=['ID', 'Oracion_ID', 'Oracion'])\n",
    "        \n",
    "        # Escribir el DataFrame al CSV (o concatenar al existente)\n",
    "        if os.path.exists(csv_file):\n",
    "            df_oraciones.to_csv(csv_file, mode='a', header=False, index=False, sep=';')\n",
    "        else:\n",
    "            df_oraciones.to_csv(csv_file, mode='w', header=True, index=False, sep=';')\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {ruta_pdf}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Mapeo de archivos a IDs y nombres\n",
    "documentos = {\n",
    "    \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\": {\"id\": 11, \"nombre\": \"PARTIDO UNIDAD POPULAR\"},\n",
    "    \"MOVIMIENTO DEMOCRACIA S√ç _Plan de trabajo_.pdf\": {\"id\": 13, \"nombre\": \"MOVIMIENTO DEMOCRACIA S√ç\"},\n",
    "    \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\": {\"id\": 15, \"nombre\": \"PARTIDO SOCIAL CRISTIANO\"}   \n",
    "}\n",
    "\n",
    "# Procesar todos los documentos\n",
    "for archivo, datos in documentos.items():\n",
    "    ruta_completa = os.path.join(pdf_directory, archivo)\n",
    "    if os.path.exists(ruta_completa):\n",
    "        if procesar_pdf(ruta_completa, datos['id'], datos['nombre']):\n",
    "            print(f\"{archivo} procesado (ID {datos['id']})\")\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {ruta_completa}\")\n",
    "\n",
    "print(\"\\nProceso completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# limpiar la columna oracion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpia y normaliza las oraciones en un archivo CSV eliminando caracteres no deseados, asegurando coherencia en los datos y ordenando por ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo oraciones.csv procesado, limpiado y ordenado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Funci√≥n para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    text = text.lower()\n",
    "    # Eliminar vi√±etas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    \n",
    "    # Eliminar (cid:...) - Referencias CID\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    \n",
    "    # Eliminar enumeraciones (n√∫meros seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la l√≠nea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "\n",
    "    # Eliminar la enumeraci√≥n de p√°gina (ejemplo: 'P√°gina 1', 'p√°g. 2', etc.)\n",
    "    text = re.sub(r'P√°gina \\d+', '', text)\n",
    "    text = re.sub(r'p√°g\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'pag\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'Page \\d+', '', text)\n",
    "    text = re.sub(r'page \\d+', '', text)\n",
    "\n",
    "    # Eliminar caracteres especiales no alfab√©ticos ni num√©ricos (como @, #, $, etc.)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Reemplazar m√∫ltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Leer el archivo CSV, limpiar el contenido de la columna \"Oracion\", ordenar por ID y guardar\n",
    "def limpiar_y_guardar_csv(csv_file):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Oracion\"\n",
    "                    row['Oracion'] = clean_content(row['Oracion'])\n",
    "                    \n",
    "                    # Verificar si la columna \"Oracion\" no est√° vac√≠a\n",
    "                    if row['Oracion']:  \n",
    "                        filas_existentes.append(row)\n",
    "        \n",
    "        # Ordenar las filas por ID (conversi√≥n a int para evitar errores de ordenaci√≥n)\n",
    "        filas_existentes.sort(key=lambda x: int(x['ID']))\n",
    "\n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Oracion_ID', 'Oracion'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo {csv_file} procesado, limpiado y ordenado correctamente.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la funci√≥n\n",
    "limpiar_y_guardar_csv('oraciones.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecta y clasifica temas clave en las oraciones extra√≠das de los documentos, agregando una columna \"Temas Clave\" en el CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CSV actualizado con √©xito. Se agregaron las columnas 'Temas Clave'.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Cargar modelo de lenguaje en espa√±ol\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Cargar el CSV con las oraciones\n",
    "df = pd.read_csv(\"oraciones.csv\", sep=\";\")\n",
    "\n",
    "# Lista ampliada de temas clave\n",
    "temas_relevantes = {\n",
    "    \"econom√≠a\", \"educaci√≥n\", \"salud\", \"seguridad\", \"empleo\",\n",
    "    \"infraestructura\", \"corrupci√≥n\", \"tecnolog√≠a\", \"ambiente\",\n",
    "    \"justicia\", \"transporte\", \"pol√≠tica\", \"desarrollo\", \"energ√≠a\",\n",
    "    \"derechos humanos\", \"igualdad\", \"innovaci√≥n\", \"turismo\",\n",
    "    \"agricultura\", \"cultura\", \"deporte\", \"finanzas\", \"inversi√≥n\",\n",
    "    \"vivienda\", \"servicios p√∫blicos\", \"ciencia\", \"medio ambiente\",\n",
    "    \"gobierno\", \"industria\", \"exportaciones\", \"importaciones\",\n",
    "    \"educaci√≥n superior\", \"sanidad\", \"movilidad\", \"inteligencia artificial\",\n",
    "    \"seguridad ciudadana\", \"crimen organizado\", \"democracia\", \"pobreza\",\n",
    "    \"sostenibilidad\", \"digitalizaci√≥n\", \"gesti√≥n p√∫blica\", \"comercio\",\n",
    "    \"cambio clim√°tico\", \"energ√≠as renovables\", \"transparencia\", \"ciberseguridad\",\n",
    "    \"salud p√∫blica\", \"gobernanza\", \"justicia social\", \"igualdad de g√©nero\",\n",
    "    \"emprendimiento\", \"industria 4.0\", \"desarrollo sostenible\", \"desastres naturales\",\n",
    "    \"reforestaci√≥n\", \"movilidad urbana\", \"biodiversidad\", \"educaci√≥n financiera\",\n",
    "    \"trabajo remoto\", \"accesibilidad\", \"industria alimentaria\", \"industria tecnol√≥gica\",\n",
    "    \"educaci√≥n digital\", \"cultura digital\", \"sociedad del conocimiento\", \n",
    "    \"banca digital\", \"teletrabajo\", \"inteligencia colectiva\", \"biotecnolog√≠a\",\n",
    "    \"blockchain\", \"fintech\", \"medicina personalizada\", \"econom√≠a circular\",\n",
    "    \"ciudades inteligentes\", \"protecci√≥n de datos\", \"energ√≠a solar\", \"transporte el√©ctrico\",\n",
    "    \"robotizaci√≥n\", \"computaci√≥n cu√°ntica\", \"espacio exterior\", \"protecci√≥n ambiental\",\n",
    "    \"seguridad en la nube\", \"movilidad el√©ctrica\", \"alimentos org√°nicos\", \"tecnolog√≠a educativa\",\n",
    "    \"agtech\", \"neurociencia\", \"edtech\", \"deep learning\", \"big data\", \"sistemas aut√≥nomos\",\n",
    "    \"tecnolog√≠a espacial\", \"cambio de paradigma\", \"smart grids\", \"ciudades sostenibles\", \n",
    "    \"ecoeficiencia\", \"energ√≠a e√≥lica\", \"tecnolog√≠as disruptivas\", \"energ√≠a geot√©rmica\",\n",
    "    \"nanotecnolog√≠a\", \"microbioma\", \"bioeconom√≠a\", \"ecoturismo\", \"industrias creativas\",\n",
    "    \"gobernanza digital\", \"energ√≠a limpia\", \"criptomonedas\", \"miner√≠a digital\", \"ciencias marinas\",\n",
    "    \"nanomateriales\", \"inteligencia emocional\", \"finanzas sostenibles\", \"educaci√≥n en l√≠nea\",\n",
    "    \"biomimicry\", \"ecoinnovaci√≥n\", \"simulaci√≥n computacional\", \"agricultura urbana\", \"cultivos inteligentes\"\n",
    "}\n",
    "\n",
    "# Funci√≥n para extraer solo los temas clave\n",
    "def extraer_temas_clave(texto):\n",
    "    if pd.isna(texto):  # Manejar valores nulos\n",
    "        return \"\"\n",
    "\n",
    "    oraciones = sent_tokenize(texto, language=\"spanish\")  # Dividir en oraciones\n",
    "    temas_detectados = set()\n",
    "\n",
    "    for oracion in oraciones:\n",
    "        # Identificar temas clave dentro de la oraci√≥n\n",
    "        temas_detectados.update({tema for tema in temas_relevantes if tema in oracion.lower()})\n",
    "\n",
    "    # Retornar los temas clave detectados como una cadena separada por coma\n",
    "    return \", \".join(temas_detectados)\n",
    "\n",
    "# Aplicar la extracci√≥n de temas clave en cada fila del DataFrame\n",
    "df[\"Temas Clave\"] = df[\"Oracion\"].apply(extraer_temas_clave)\n",
    "\n",
    "# Guardar el nuevo CSV con la nueva columna 'Temas Clave' sin eliminar datos anteriores\n",
    "df.to_csv(\"oraciones_actualizado.csv\", index=False, sep=\";\")\n",
    "\n",
    "print(\" CSV actualizado con √©xito. Se agregaron las columnas 'Temas Clave'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words, tokenizar,lemmatizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpia, lematiza y elimina stopwords en las oraciones procesadas, reduciendo dimensionalidad y normalizando los datos antes de su an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo procesado y guardado correctamente en: oraciones_procesadas.csv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Inicializar el lematizador de SpaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Obtener las stopwords en espa√±ol de NLTK\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "# Funci√≥n para lematizar y eliminar stopwords\n",
    "def lematizar_y_eliminar_stopwords(texto):\n",
    "    doc = nlp(texto)\n",
    "    # Lematizar y eliminar stopwords\n",
    "    return ' '.join([token.lemma_ for token in doc if token.is_alpha and token.lemma_ not in stop_words])\n",
    "\n",
    "# Limpiar contenido eliminando puntuaciones y n√∫meros\n",
    "def clean_content(texto):\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto)  # Eliminar puntuaciones\n",
    "    texto = re.sub(r'\\d+', '', texto)      # Eliminar n√∫meros\n",
    "    return texto.lower()\n",
    "\n",
    "# Funci√≥n para procesar el CSV\n",
    "def limpiar_y_guardar_csv(csv_file, output_csv):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Oracion\"\n",
    "                    if 'Oracion' in row:\n",
    "                        row['Oracion'] = clean_content(row['Oracion'])\n",
    "                        # Procesar el contenido: lematizar y eliminar stopwords\n",
    "                        row['Oracion'] = lematizar_y_eliminar_stopwords(row['Oracion'])\n",
    "                    \n",
    "                    # Verificar si la columna \"Oracion\" no est√° vac√≠a\n",
    "                    if row['Oracion']:  # Si no est√° vac√≠o o solo contiene espacios\n",
    "                        filas_existentes.append(row)\n",
    "        \n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Oracion_ID', 'Oracion', 'Temas Clave'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo procesado y guardado correctamente en: {output_csv}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la funci√≥n\n",
    "input_csv = 'oraciones_actualizado.csv'\n",
    "output_csv = 'oraciones_procesadas.csv'\n",
    "limpiar_y_guardar_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fusiona oraciones procesadas con informaci√≥n de candidatos mediante ID, asegurando consistencia en el formato y guardando el resultado en un CSV consolidado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo combinado guardado como 'oraciones_procesadas_completo.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los archivos CSV asegurando el delimitador correcto\n",
    "df_oraciones = pd.read_csv('oraciones_procesadas.csv', delimiter=';')\n",
    "df_candidatos = pd.read_csv('candidatos.csv', delimiter=';')\n",
    "\n",
    "# Limpiar la columna 'ID' eliminando espacios y asegurando que solo contenga n√∫meros\n",
    "df_oraciones['ID'] = df_oraciones['ID'].astype(str).str.extract('(\\d+)').astype(float).astype('Int64')\n",
    "df_candidatos['ID'] = df_candidatos['ID'].astype(str).str.extract('(\\d+)').astype(float).astype('Int64')\n",
    "\n",
    "# Realizar la fusi√≥n de datos usando 'ID' como clave\n",
    "df_completo = df_oraciones.merge(df_candidatos, on='ID', how='left')\n",
    "\n",
    "# Guardar el nuevo CSV con los datos combinados\n",
    "df_completo.to_csv('oraciones_procesadas_completo.csv', sep=';', index=False)\n",
    "\n",
    "print(\"Archivo combinado guardado como 'oraciones_procesadas_completo.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enbeddings Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genera embeddings de las oraciones utilizando BERT, optimizando el c√°lculo con GPU y almacenando los resultados en .pkl para su posterior uso en an√°lisis sem√°ntico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Primeras filas de df:\n",
      "   ID Oracion_ID                                            Oracion  \\\n",
      "0   1          1  objetivo general alcanzar buen vivir democraci...   \n",
      "1   1          2                       objetivo espec√≠fico objetivo   \n",
      "2   1          3                       justicia buen vivir objetivo   \n",
      "3   1          4     justicia alcanzar paz seguridad orden objetivo   \n",
      "4   1          5  justicia bienestar econ√≥mico igualdad oportuni...   \n",
      "\n",
      "                               Temas Clave                      Partido  \\\n",
      "0  democracia, pol√≠tica, justicia, cultura  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "1                                      NaN  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "2                                 justicia  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "3                      justicia, seguridad  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "4                       justicia, igualdad  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "\n",
      "  CandidatoPresidente CandidatoVicePresidente ListaPolitica  \n",
      "0      LUISA GONZALEZ             DIEGO BORJA          5-33  \n",
      "1      LUISA GONZALEZ             DIEGO BORJA          5-33  \n",
      "2      LUISA GONZALEZ             DIEGO BORJA          5-33  \n",
      "3      LUISA GONZALEZ             DIEGO BORJA          5-33  \n",
      "4      LUISA GONZALEZ             DIEGO BORJA          5-33  \n",
      "Primeras filas de df2:\n",
      "    id  oracion_id                                   oracion_original  \\\n",
      "0  AG1           1  Andrea Gonz√°lez Nader, candidata presidencial ...   \n",
      "1  AG1           2  Propone una reducci√≥n del aparato estatal medi...   \n",
      "2  AG1           3  En materia de seguridad, destaca la necesidad ...   \n",
      "3  AG1           4  Adem√°s, promueve una reforma constitucional si...   \n",
      "4  AG1           5  En el √°mbito de salud, propone un enfoque prev...   \n",
      "\n",
      "                                      oracion_limpia  \\\n",
      "0  andrea gonzalez nader candidata presidencial p...   \n",
      "1  propone una reduccion del aparato estatal medi...   \n",
      "2  en materia de seguridad destaca la necesidad d...   \n",
      "3  ademas promueve una reforma constitucional sin...   \n",
      "4  en el ambito de salud propone un enfoque preve...   \n",
      "\n",
      "                                oracion_sinStopWords       presidente  \\\n",
      "0  andrea gonzalez nader candidata presidencial s...  ANDREA GONZALEZ   \n",
      "1  propone reduccion aparato estatal mediante eli...  ANDREA GONZALEZ   \n",
      "2  materia seguridad destaca necesidad retomar co...  ANDREA GONZALEZ   \n",
      "3  ademas promueve reforma constitucional recurri...  ANDREA GONZALEZ   \n",
      "4  ambito salud propone enfoque preventivo lugar ...  ANDREA GONZALEZ   \n",
      "\n",
      "  vicepresidente  lista                                  Partido  \n",
      "0   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO  \n",
      "1   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO  \n",
      "2   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO  \n",
      "3   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO  \n",
      "4   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO  \n",
      "Embeddings en df:\n",
      "                                             Oracion  \\\n",
      "0  objetivo general alcanzar buen vivir democraci...   \n",
      "1                       objetivo espec√≠fico objetivo   \n",
      "2                       justicia buen vivir objetivo   \n",
      "3     justicia alcanzar paz seguridad orden objetivo   \n",
      "4  justicia bienestar econ√≥mico igualdad oportuni...   \n",
      "\n",
      "                                   embedding_Oracion  \n",
      "0  [[-0.33315042, 0.08386034, 0.17690535, 0.11592...  \n",
      "1  [[-0.52293205, -0.009774199, 0.16899616, 0.168...  \n",
      "2  [[-0.5875125, -0.185553, -0.051314298, 0.23033...  \n",
      "3  [[-0.66177183, -0.045373067, 0.24473608, 0.280...  \n",
      "4  [[-0.40853977, -0.14116393, 0.25238565, 0.2702...  \n",
      "Embeddings en df2:\n",
      "                                oracion_sinStopWords  \\\n",
      "0  andrea gonzalez nader candidata presidencial s...   \n",
      "1  propone reduccion aparato estatal mediante eli...   \n",
      "2  materia seguridad destaca necesidad retomar co...   \n",
      "3  ademas promueve reforma constitucional recurri...   \n",
      "4  ambito salud propone enfoque preventivo lugar ...   \n",
      "\n",
      "                      embedding_oracion_sinStopWords  \n",
      "0  [[-0.42224392, -0.14215767, 0.43130305, 0.2447...  \n",
      "1  [[-0.29042023, 0.16923054, 0.4194137, 0.332981...  \n",
      "2  [[-0.5043017, -0.0928126, 0.359641, 0.17502746...  \n",
      "3  [[-0.36289763, 0.026272643, 0.3643691, 0.25889...  \n",
      "4  [[-0.32233056, 0.081609026, 0.14803693, 0.3004...  \n",
      "Embeddings guardados en archivos .pkl.\n"
     ]
    }
   ],
   "source": [
    "    import pandas as pd\n",
    "    from transformers import BertTokenizer, BertModel\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "\n",
    "    # Detectar dispositivo: usa GPU si est√° disponible\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "    # Cargar los archivos CSV\n",
    "    df = pd.read_csv('oraciones_procesadas_completo.csv', delimiter=';')\n",
    "    df2 = pd.read_csv('./data/interview/elecciones_oraciones.csv', sep=\",\")\n",
    "\n",
    "    # Ver los primeros registros de ambos archivos para comprobar que se cargaron correctamente\n",
    "    print(\"Primeras filas de df:\")\n",
    "    print(df.head())\n",
    "    print(\"Primeras filas de df2:\")\n",
    "    print(df2.head())\n",
    "\n",
    "    # Cargar el tokenizador y el modelo BERT preentrenado y mover el modelo al dispositivo (GPU o CPU)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "    # Funci√≥n para obtener el embedding de una oraci√≥n utilizando la GPU\n",
    "    def obtener_embedding(texto):\n",
    "        # Asegurarse de que el texto sea una cadena\n",
    "        texto = str(texto)\n",
    "        # Tokenizar el texto y mover los tensores al dispositivo\n",
    "        inputs = tokenizer(texto, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "        # Obtener los embeddings de BERT\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Tomar el embedding del token [CLS] (primer token) y moverlo a la CPU para convertirlo en numpy\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        return embedding\n",
    "\n",
    "    # Generar embeddings para las oraciones en el primer DataFrame (df)\n",
    "    df['embedding_Oracion'] = df['Oracion'].apply(lambda x: obtener_embedding(x))\n",
    "    # Generar embeddings para las oraciones en el segundo DataFrame (df2)\n",
    "    df2['embedding_oracion_sinStopWords'] = df2['oracion_sinStopWords'].apply(lambda x: obtener_embedding(x))\n",
    "\n",
    "    # Ver los primeros resultados con los embeddings generados\n",
    "    print(\"Embeddings en df:\")\n",
    "    print(df[['Oracion', 'embedding_Oracion']].head())\n",
    "\n",
    "    print(\"Embeddings en df2:\")\n",
    "    print(df2[['oracion_sinStopWords', 'embedding_oracion_sinStopWords']].head())\n",
    "\n",
    "    # Guardar los DataFrames con los embeddings generados en formato .pkl\n",
    "    with open('oraciones_con_embeddings_completo.pkl', 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "\n",
    "    with open('./data/interview/elecciones_oraciones_con_embeddings.pkl', 'wb') as f:\n",
    "        pickle.dump(df2, f)\n",
    "\n",
    "    print(\"Embeddings guardados en archivos .pkl.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[‚úÖ] √çndice FAISS de candidatos guardado con √©xito.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Cargar CSV de candidatos\n",
    "# ---------------------------\n",
    "candidatos_df = pd.read_csv(\"candidatos.csv\", delimiter=\";\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Cargar el modelo BERT para embeddings\n",
    "# ---------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "def obtener_embedding(texto):\n",
    "    texto = str(texto)\n",
    "    inputs = tokenizer(texto, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten()\n",
    "    return embedding\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Generar embeddings de candidatos\n",
    "# ---------------------------\n",
    "candidatos_df[\"embedding\"] = candidatos_df[\"CandidatoPresidente\"].apply(obtener_embedding)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Crear el √≠ndice FAISS\n",
    "# ---------------------------\n",
    "def construir_index_faiss(df, embedding_col):\n",
    "    embeddings_matrix = np.vstack(df[embedding_col].values).astype('float32')\n",
    "    dim = embeddings_matrix.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings_matrix)\n",
    "    return index\n",
    "\n",
    "index_candidatos = construir_index_faiss(candidatos_df, \"embedding\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Guardar el √≠ndice y el DataFrame en archivos\n",
    "# ---------------------------\n",
    "with open(\"candidatos_faiss_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(index_candidatos, f)\n",
    "\n",
    "with open(\"candidatos_dataframe.pkl\", \"wb\") as f:\n",
    "    pickle.dump(candidatos_df, f)\n",
    "\n",
    "print(\"[‚úÖ] √çndice FAISS de candidatos guardado con √©xito.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genera embeddings con SentenceTransformer, indexa las oraciones en FAISS para b√∫squeda sem√°ntica y permite recuperar informaci√≥n relevante con consultas en lenguaje natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df962099739240fa91048bf43a4bbb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings y FAISS guardados exitosamente.\n",
      "{'ID': 4, 'Oracion_ID': '993', 'Oracion': 'eficiencia energ√©tico implementar pol√≠tica eficiencia energ√©tico sector clave industria transporte construcci√≥n promover uso tecnolog√≠a reducir consumo energ√≠a', 'Temas Clave': 'energ√≠a, transporte, ciencia, tecnolog√≠a, pol√≠tica, industria', 'Partido': 'MOVIMIENTO CENTRO DEMOCR√ÅTICO', 'CandidatoPresidente': 'JIMMY JAIRALA VALLAZZA', 'CandidatoVicePresidente': 'LUCIA VALLECILLA SUAREZ', 'ListaPolitica': '1', 'Distancia': 0.33702278}\n",
      "{'ID': 14, 'Oracion_ID': '811', 'Oracion': 'adem√°s realizar campa√±a nacional eficiencia ahorro energ√©tico √©nfasis sector comercial residencial industrial institucional', 'Temas Clave': 'industria, ciencia', 'Partido': 'PARTIDO AVANZA', 'CandidatoPresidente': 'LUIS FELIPE TILLERIA', 'CandidatoVicePresidente': 'KARLA PAULINA ROSERO', 'ListaPolitica': '8', 'Distancia': 0.38165388}\n",
      "{'ID': 14, 'Oracion_ID': '827', 'Oracion': 'campa√±a nacional eficiencia ahorro energ√©tico promover pr√°ctica eficiencia ahorro energ√©tico sector', 'Temas Clave': 'ciencia', 'Partido': 'PARTIDO AVANZA', 'CandidatoPresidente': 'LUIS FELIPE TILLERIA', 'CandidatoVicePresidente': 'KARLA PAULINA ROSERO', 'ListaPolitica': '8', 'Distancia': 0.4163392}\n",
      "{'ID': 8, 'Oracion_ID': '433', 'Oracion': 'impulso inversi√≥n privado gobierno deber crear entorno atractivo inversi√≥n privado sector energ√©tico', 'Temas Clave': 'gobierno, inversi√≥n', 'Partido': 'MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID', 'CandidatoPresidente': 'VICTOR ARAUS', 'CandidatoVicePresidente': 'CRISTINA CARRERA', 'ListaPolitica': '4', 'Distancia': 0.42793632}\n",
      "{'ID': 1, 'Oracion_ID': '533', 'Oracion': 'intervenir vivienda promover eficiencia energ√©tico', 'Temas Clave': 'vivienda, ciencia', 'Partido': 'REVOLUCI√ìN CIUDADANA - RETO', 'CandidatoPresidente': 'LUISA GONZALEZ', 'CandidatoVicePresidente': 'DIEGO BORJA', 'ListaPolitica': '5-33', 'Distancia': 0.42992368}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Verificar si hay GPU disponible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Cargar los archivos CSV\n",
    "df = pd.read_csv('oraciones_procesadas_completo.csv', delimiter=';')\n",
    "df2=pd.read_csv('./data/interview/elecciones_oraciones.csv', sep=\",\")\n",
    "\n",
    "# Inicializar el modelo BERT y moverlo a GPU si est√° disponible\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "# Crear embeddings para todas las oraciones\n",
    "embeddings = model.encode(df['Oracion'].tolist(), show_progress_bar=True, device=device)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Crear un √≠ndice FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Guardar los embeddings e √≠ndice FAISS\n",
    "np.save('embeddings.npy', embeddings)\n",
    "faiss.write_index(index, 'faiss_index.index')\n",
    "\n",
    "# Guardar datos procesados en un archivo pickle\n",
    "with open('sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(orient='records'), f)\n",
    "\n",
    "print(\"Embeddings y FAISS guardados exitosamente.\")\n",
    "\n",
    "# Cargar spaCy para espa√±ol\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower(), language='spanish')\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def query_faiss(query, top_k=5):\n",
    "    cleaned_query = preprocess_text(query)\n",
    "    query_embedding = model.encode([cleaned_query], device=device).astype('float32')\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    \n",
    "    with open('sentences.pkl', 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(top_k):\n",
    "        result = sentences[I[0][i]]\n",
    "        results.append({\n",
    "            'ID': result['ID'],\n",
    "            'Oracion_ID':result['Oracion_ID'],\n",
    "            'Oracion': result['Oracion'],\n",
    "            'Temas Clave': result['Temas Clave'],\n",
    "            'Partido': result['Partido'],\n",
    "            'CandidatoPresidente': result['CandidatoPresidente'],\n",
    "            'CandidatoVicePresidente': result['CandidatoVicePresidente'],\n",
    "            'ListaPolitica': result['ListaPolitica'],\n",
    "            'Distancia': D[0][i]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Ejemplo de uso\n",
    "query = \"¬øC√≥mo mejorar la eficiencia energ√©tica en la industria?\"\n",
    "results = query_faiss(query)\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga DataFrames con embeddings desde archivos .pkl para su reutilizaci√≥n en tareas de b√∫squeda sem√°ntica y an√°lisis de similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del primer DataFrame:\n",
      "   ID Oracion_ID                                            Oracion  \\\n",
      "0   1          1  objetivo general alcanzar buen vivir democraci...   \n",
      "1   1          2                       objetivo espec√≠fico objetivo   \n",
      "2   1          3                       justicia buen vivir objetivo   \n",
      "3   1          4     justicia alcanzar paz seguridad orden objetivo   \n",
      "4   1          5  justicia bienestar econ√≥mico igualdad oportuni...   \n",
      "\n",
      "                               Temas Clave                      Partido  \\\n",
      "0  democracia, pol√≠tica, justicia, cultura  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "1                                      NaN  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "2                                 justicia  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "3                      justicia, seguridad  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "4                       justicia, igualdad  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "\n",
      "  CandidatoPresidente CandidatoVicePresidente ListaPolitica  \\\n",
      "0      LUISA GONZALEZ             DIEGO BORJA          5-33   \n",
      "1      LUISA GONZALEZ             DIEGO BORJA          5-33   \n",
      "2      LUISA GONZALEZ             DIEGO BORJA          5-33   \n",
      "3      LUISA GONZALEZ             DIEGO BORJA          5-33   \n",
      "4      LUISA GONZALEZ             DIEGO BORJA          5-33   \n",
      "\n",
      "                                   embedding_Oracion  \n",
      "0  [[-0.33315042, 0.08386034, 0.17690535, 0.11592...  \n",
      "1  [[-0.52293205, -0.009774199, 0.16899616, 0.168...  \n",
      "2  [[-0.5875125, -0.185553, -0.051314298, 0.23033...  \n",
      "3  [[-0.66177183, -0.045373067, 0.24473608, 0.280...  \n",
      "4  [[-0.40853977, -0.14116393, 0.25238565, 0.2702...  \n",
      "\n",
      "Primeras filas del segundo DataFrame:\n",
      "    id  oracion_id                                   oracion_original  \\\n",
      "0  AG1           1  Andrea Gonz√°lez Nader, candidata presidencial ...   \n",
      "1  AG1           2  Propone una reducci√≥n del aparato estatal medi...   \n",
      "2  AG1           3  En materia de seguridad, destaca la necesidad ...   \n",
      "3  AG1           4  Adem√°s, promueve una reforma constitucional si...   \n",
      "4  AG1           5  En el √°mbito de salud, propone un enfoque prev...   \n",
      "\n",
      "                                      oracion_limpia  \\\n",
      "0  andrea gonzalez nader candidata presidencial p...   \n",
      "1  propone una reduccion del aparato estatal medi...   \n",
      "2  en materia de seguridad destaca la necesidad d...   \n",
      "3  ademas promueve una reforma constitucional sin...   \n",
      "4  en el ambito de salud propone un enfoque preve...   \n",
      "\n",
      "                                oracion_sinStopWords       presidente  \\\n",
      "0  andrea gonzalez nader candidata presidencial s...  ANDREA GONZALEZ   \n",
      "1  propone reduccion aparato estatal mediante eli...  ANDREA GONZALEZ   \n",
      "2  materia seguridad destaca necesidad retomar co...  ANDREA GONZALEZ   \n",
      "3  ademas promueve reforma constitucional recurri...  ANDREA GONZALEZ   \n",
      "4  ambito salud propone enfoque preventivo lugar ...  ANDREA GONZALEZ   \n",
      "\n",
      "  vicepresidente  lista                                  Partido  \\\n",
      "0   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO   \n",
      "1   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO   \n",
      "2   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO   \n",
      "3   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO   \n",
      "4   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO   \n",
      "\n",
      "                      embedding_oracion_sinStopWords  \n",
      "0  [[-0.42224392, -0.14215767, 0.43130305, 0.2447...  \n",
      "1  [[-0.29042023, 0.16923054, 0.4194137, 0.332981...  \n",
      "2  [[-0.5043017, -0.0928126, 0.359641, 0.17502746...  \n",
      "3  [[-0.36289763, 0.026272643, 0.3643691, 0.25889...  \n",
      "4  [[-0.32233056, 0.081609026, 0.14803693, 0.3004...  \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el DataFrame del primer CSV (por ejemplo, que contiene la columna 'embedding_Oracion')\n",
    "with open('oraciones_con_embeddings_completo.pkl', 'rb') as f:\n",
    "    df1 = pickle.load(f)\n",
    "\n",
    "# Cargar el DataFrame del segundo CSV (por ejemplo, que contiene la columna 'embedding_oracion_sinStopWords')\n",
    "with open('./data/interview/elecciones_oraciones_con_embeddings.pkl', 'rb') as f:\n",
    "    df2 = pickle.load(f)\n",
    "\n",
    "# Verificar la carga\n",
    "print(\"Primeras filas del primer DataFrame:\")\n",
    "print(df1.head())\n",
    "print(\"\\nPrimeras filas del segundo DataFrame:\")\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea √≠ndices FAISS con embeddings de oraciones usando IndexFlatL2, optimizando la b√∫squeda sem√°ntica y guardando los √≠ndices para consultas futuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El √≠ndice FAISS del primer DataFrame contiene 13759 vectores.\n",
      "El √≠ndice FAISS del segundo DataFrame contiene 7753 vectores.\n",
      "√çndice FAISS del primer DataFrame guardado en 'faiss_index_oraciones_procesadas.index'.\n",
      "√çndice FAISS del segundo DataFrame guardado en 'faiss_index_elecciones_oraciones.index'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# --- Para el primer DataFrame ---\n",
    "# Extraer y apilar los embeddings\n",
    "embeddings1 = np.vstack(df1['embedding_Oracion'].values).astype('float32')\n",
    "dim1 = embeddings1.shape[1]\n",
    "\n",
    "# Crear el √≠ndice FAISS (usamos IndexFlatL2 para b√∫squeda por distancia euclidiana)\n",
    "index1 = faiss.IndexFlatL2(dim1)\n",
    "index1.add(embeddings1)\n",
    "print(f\"El √≠ndice FAISS del primer DataFrame contiene {index1.ntotal} vectores.\")\n",
    "\n",
    "# --- Para el segundo DataFrame ---\n",
    "# Extraer y apilar los embeddings\n",
    "embeddings2 = np.vstack(df2['embedding_oracion_sinStopWords'].values).astype('float32')\n",
    "dim2 = embeddings2.shape[1]\n",
    "\n",
    "# Crear el √≠ndice FAISS para el segundo conjunto de embeddings\n",
    "index2 = faiss.IndexFlatL2(dim2)\n",
    "index2.add(embeddings2)\n",
    "print(f\"El √≠ndice FAISS del segundo DataFrame contiene {index2.ntotal} vectores.\")\n",
    "\n",
    "# Guardar el √≠ndice del primer DataFrame\n",
    "faiss.write_index(index1, 'faiss_index_oraciones_procesadas.index')\n",
    "print(\"√çndice FAISS del primer DataFrame guardado en 'faiss_index_oraciones_procesadas.index'.\")\n",
    "\n",
    "# Guardar el √≠ndice del segundo DataFrame\n",
    "faiss.write_index(index2, 'faiss_index_elecciones_oraciones.index')\n",
    "print(\"√çndice FAISS del segundo DataFrame guardado en 'faiss_index_elecciones_oraciones.index'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruye √≠ndices FAISS desde embeddings preexistentes y permite b√∫squedas sem√°nticas en tiempo real utilizando BERT para generar representaciones de consultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se agregaron 13759 vectores al √≠ndice FAISS.\n",
      "Se agregaron 7753 vectores al √≠ndice FAISS.\n",
      "\n",
      "--- Consulta en el primer √≠ndice (df1) ---\n",
      "\n",
      "Consulta: \"eduacion\"\n",
      "Indices encontrados: [ 4651  4647  8323  8647 12534]\n",
      "Distancias: [12.340698 13.612962 13.612962 13.612962 13.612962]\n",
      "\n",
      "Resultados:\n",
      "       ID Oracion_ID         Oracion Temas Clave  \\\n",
      "4651    3       1499  concienciaci√≥n     ciencia   \n",
      "4647    3       1495       educaci√≥n   educaci√≥n   \n",
      "8323    7        510       educaci√≥n   educaci√≥n   \n",
      "8647    7        938       educaci√≥n   educaci√≥n   \n",
      "12534  14        586       educaci√≥n   educaci√≥n   \n",
      "\n",
      "                                                 Partido  \\\n",
      "4651                       PARTIDO IZQUIERDA DEMOCR√ÅTICA   \n",
      "4647                       PARTIDO IZQUIERDA DEMOCR√ÅTICA   \n",
      "8323   MOVIMIENTO AMIGO, ACCI√ìN MOVILIZADORA INDEPEND...   \n",
      "8647   MOVIMIENTO AMIGO, ACCI√ìN MOVILIZADORA INDEPEND...   \n",
      "12534                                     PARTIDO AVANZA   \n",
      "\n",
      "        CandidatoPresidente   CandidatoVicePresidente ListaPolitica  \\\n",
      "4651       CARLOS RABASCALL  ALEJANDRA RIVAS MANTILLA            12   \n",
      "4647       CARLOS RABASCALL  ALEJANDRA RIVAS MANTILLA            12   \n",
      "8323        JUAN IVAN CUEVA            CRISTINA REYES            16   \n",
      "8647        JUAN IVAN CUEVA            CRISTINA REYES            16   \n",
      "12534  LUIS FELIPE TILLERIA      KARLA PAULINA ROSERO             8   \n",
      "\n",
      "                                       embedding_Oracion  \n",
      "4651   [[-0.37460178, -0.08777584, -0.20984206, -0.14...  \n",
      "4647   [[-0.2738171, 0.007819946, 0.054867085, -0.142...  \n",
      "8323   [[-0.2738171, 0.007819946, 0.054867085, -0.142...  \n",
      "8647   [[-0.2738171, 0.007819946, 0.054867085, -0.142...  \n",
      "12534  [[-0.2738171, 0.007819946, 0.054867085, -0.142...  \n",
      "\n",
      "--- Consulta en el segundo √≠ndice (df2) ---\n",
      "\n",
      "Consulta: \"bajar iva.\"\n",
      "Indices encontrados: [1102 4448 3400 4148 5540]\n",
      "Distancias: [40.086105 40.801044 42.0157   43.03201  43.514687]\n",
      "\n",
      "Resultados:\n",
      "       id  oracion_id                                 oracion_original  \\\n",
      "1102  FT2          69                      Pero bueno, fuera de broma.   \n",
      "4448  LT3         256        La palabra en ingl√©s es rogue, R-O-G-Q-E.   \n",
      "3400  JC3          83         ¬øElla podr√≠a convertirse en un aminista?   \n",
      "4148  LT2          81                       Vamos a la contra r√©plica.   \n",
      "5540  DN1         128  Y nosotros tenemos que estar abiertos para eso.   \n",
      "\n",
      "                                      oracion_limpia  \\\n",
      "1102                       pero bueno fuera de broma   \n",
      "4448         la palabra en ingles es rogue r o g q e   \n",
      "3400          ella podria convertirse en un aminista   \n",
      "4148                       vamos a la contra replica   \n",
      "5540  y nosotros tenemos que estar abiertos para eso   \n",
      "\n",
      "             oracion_sinStopWords          presidente    vicepresidente  \\\n",
      "1102                  bueno broma  FRANCESCO TABACCHI  BLANCA SACANCELA   \n",
      "4448   palabra ingles rogue r g q                 NaN               NaN   \n",
      "3400  podria convertirse aminista                 NaN               NaN   \n",
      "4148                vamos replica                 NaN               NaN   \n",
      "5540                     abiertos   DANIEL NOBOA AZIN  MARIA JOSE PINTO   \n",
      "\n",
      "      lista                                      Partido  \\\n",
      "1102   21.0       MOVIMIENTO CREO, CREANDO OPORTUNIDADES   \n",
      "4448    NaN                                          NaN   \n",
      "3400    NaN                                          NaN   \n",
      "4148    NaN                                          NaN   \n",
      "5540    7.0  MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN   \n",
      "\n",
      "                         embedding_oracion_sinStopWords  \n",
      "1102  [[-0.5601457, -0.07494587, -0.04576185, -0.137...  \n",
      "4448  [[-0.56483746, -0.007231636, 0.07722815, -0.11...  \n",
      "3400  [[-0.6505401, -0.12425378, -0.21291989, -0.107...  \n",
      "4148  [[-0.41923407, -0.108675756, -0.20688075, 0.03...  \n",
      "5540  [[-0.5437605, 0.036334, -0.060376026, -0.25117...  \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import faiss\n",
    "\n",
    "# Cargar el tokenizador y el modelo BERT (usa el mismo que usaste para generar los embeddings)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def obtener_embedding(texto):\n",
    "    \"\"\"\n",
    "    Dada una cadena de texto, devuelve el embedding correspondiente usando BERT.\n",
    "    Se utiliza el embedding del token [CLS] como representaci√≥n.\n",
    "    \"\"\"\n",
    "    texto = str(texto)  # asegurar que sea string\n",
    "    inputs = tokenizer(texto, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Se toma el embedding del token [CLS] (posici√≥n 0) y se aplana a 1D (768,)\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "    return embedding\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Cargar los DataFrames con embeddings desde los archivos .pkl\n",
    "# ---------------------------------------------------------------------------\n",
    "# Ejemplo para el primer CSV (ajusta el nombre del archivo seg√∫n corresponda)\n",
    "with open('oraciones_con_embeddings_completo.pkl', 'rb') as f:\n",
    "    df1 = pickle.load(f)\n",
    "    \n",
    "# Ejemplo para el segundo CSV\n",
    "with open('./data/interview/elecciones_oraciones_con_embeddings.pkl', 'rb') as f:\n",
    "    df2 = pickle.load(f)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Reconstruir los √≠ndices FAISS a partir de los embeddings almacenados en los DataFrames\n",
    "# ---------------------------------------------------------------------------\n",
    "def construir_index_faiss(df, embedding_col):\n",
    "    \"\"\"\n",
    "    Dado un DataFrame y el nombre de la columna que contiene los embeddings,\n",
    "    crea y devuelve un √≠ndice FAISS usando IndexFlatL2.\n",
    "    \"\"\"\n",
    "    # Extraer y apilar los embeddings en una matriz NumPy de tipo float32\n",
    "    embeddings_matrix = np.vstack(df[embedding_col].values).astype('float32')\n",
    "    dim = embeddings_matrix.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings_matrix)\n",
    "    print(f\"Se agregaron {index.ntotal} vectores al √≠ndice FAISS.\")\n",
    "    return index\n",
    "\n",
    "# Para el primer DataFrame, asumimos que la columna de embeddings se llama 'embedding_Oracion'\n",
    "index1 = construir_index_faiss(df1, 'embedding_Oracion')\n",
    "\n",
    "# Para el segundo DataFrame, asumimos que la columna de embeddings se llama 'embedding_oracion_sinStopWords'\n",
    "index2 = construir_index_faiss(df2, 'embedding_oracion_sinStopWords')\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Funci√≥n para hacer consultas en un √≠ndice FAISS\n",
    "# ---------------------------------------------------------------------------\n",
    "def hacer_consulta(index, df, query_text, k=5):\n",
    "    \"\"\"\n",
    "    Dada una consulta en forma de texto:\n",
    "      - Calcula su embedding.\n",
    "      - Busca los k vecinos m√°s cercanos en el √≠ndice FAISS.\n",
    "      - Recupera y muestra la metadata de los registros encontrados.\n",
    "    \n",
    "    Par√°metros:\n",
    "      index: √çndice FAISS.\n",
    "      df: DataFrame original con la metadata, donde el orden coincide con el √≠ndice.\n",
    "      query_text: Texto de consulta.\n",
    "      k: N√∫mero de vecinos a recuperar.\n",
    "    \"\"\"\n",
    "    # Obtener el embedding de la consulta y asegurarse de que sea float32\n",
    "    embedding_query = obtener_embedding(query_text).astype('float32')\n",
    "    # Expandir dimensiones para que sea (1, dim)\n",
    "    query_vector = np.expand_dims(embedding_query, axis=0)\n",
    "    \n",
    "    # Buscar en el √≠ndice\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"\\nConsulta: \\\"{query_text}\\\"\")\n",
    "    print(\"Indices encontrados:\", indices[0])\n",
    "    print(\"Distancias:\", distances[0])\n",
    "    print(\"\\nResultados:\")\n",
    "    # Se asume que el DataFrame tiene la metadata deseada en las columnas originales.\n",
    "    # Por ejemplo, para df1 se podr√≠an mostrar 'Oracion', 'CandidatoPresidente', etc.\n",
    "    resultados = df.iloc[indices[0]]\n",
    "    print(resultados)\n",
    "    return distances, indices\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. Ejemplo de consulta\n",
    "# ---------------------------------------------------------------------------\n",
    "# Consulta en el primer √≠ndice (df1) usando la columna 'embedding_Oracion'\n",
    "query_text1 = \"eduacion\"\n",
    "print(\"\\n--- Consulta en el primer √≠ndice (df1) ---\")\n",
    "_ = hacer_consulta(index1, df1, query_text1, k=5)\n",
    "\n",
    "# Consulta en el segundo √≠ndice (df2) usando la columna 'embedding_oracion_sinStopWords'\n",
    "query_text2 = \"bajar iva.\"\n",
    "print(\"\\n--- Consulta en el segundo √≠ndice (df2) ---\")\n",
    "_ = hacer_consulta(index2, df2, query_text2, k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea un √≠ndice FAISS con embeddings de candidatos generados por BERT, permitiendo b√∫squedas sem√°nticas de candidatos pol√≠ticos a partir de consultas en lenguaje natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Consulta ---\n",
      "Distancia obtenida: 32.213364\n",
      "√çndice en el √≠ndice FAISS: 18\n",
      "Candidato encontrado: {'nombre': 'ANDREA GONZALEZ', 'tipo': 'presidente', 'whois': 'Andrea Gonz√°lez Nader es una activista medioambiental y pol√≠tica ecuatoriana de 37 a√±os, originaria de Guayaquil. Ha trabajado como profesora en instituciones educativas como los colegios Delta-Torremar, Cruz del Sur y la Unidad Educativa Biling√ºe Delta entre 2007 y 2015. Adem√°s, ha colaborado con organizaciones medioambientales como la Fundaci√≥n La Iguana y ha liderado proyectos de paisajismo sostenible y manejo integral de desechos. En 2023, fue candidata a la vicepresidencia de la Rep√∫blica junto al periodista Fernando Villavicencio, y posteriormente acompa√±√≥ al periodista Christian Zurita en el mismo proceso electoral. En agosto de 2024, fue proclamada candidata presidencial por el Partido Sociedad Patri√≥tica (PSP) para las elecciones de 2025.'}\n",
      "Resultado: Andrea Gonz√°lez Nader es una activista medioambiental y pol√≠tica ecuatoriana de 37 a√±os, originaria de Guayaquil. Ha trabajado como profesora en instituciones educativas como los colegios Delta-Torremar, Cruz del Sur y la Unidad Educativa Biling√ºe Delta entre 2007 y 2015. Adem√°s, ha colaborado con organizaciones medioambientales como la Fundaci√≥n La Iguana y ha liderado proyectos de paisajismo sostenible y manejo integral de desechos. En 2023, fue candidata a la vicepresidencia de la Rep√∫blica junto al periodista Fernando Villavicencio, y posteriormente acompa√±√≥ al periodista Christian Zurita en el mismo proceso electoral. En agosto de 2024, fue proclamada candidata presidencial por el Partido Sociedad Patri√≥tica (PSP) para las elecciones de 2025.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Andrea Gonz√°lez Nader es una activista medioambiental y pol√≠tica ecuatoriana de 37 a√±os, originaria de Guayaquil. Ha trabajado como profesora en instituciones educativas como los colegios Delta-Torremar, Cruz del Sur y la Unidad Educativa Biling√ºe Delta entre 2007 y 2015. Adem√°s, ha colaborado con organizaciones medioambientales como la Fundaci√≥n La Iguana y ha liderado proyectos de paisajismo sostenible y manejo integral de desechos. En 2023, fue candidata a la vicepresidencia de la Rep√∫blica junto al periodista Fernando Villavicencio, y posteriormente acompa√±√≥ al periodista Christian Zurita en el mismo proceso electoral. En agosto de 2024, fue proclamada candidata presidencial por el Partido Sociedad Patri√≥tica (PSP) para las elecciones de 2025.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el archivo CSV con el delimitador ';'\n",
    "df = pd.read_csv('candidatos.csv', delimiter=';')\n",
    "\n",
    "# Cargar el modelo BERT y su tokenizer (en este ejemplo se usa 'bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Funci√≥n para normalizar texto: convierte a min√∫sculas y elimina acentos\n",
    "def normalizar_texto(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = unicodedata.normalize('NFD', texto)\n",
    "    texto = ''.join([c for c in texto if unicodedata.category(c) != 'Mn'])\n",
    "    return texto\n",
    "\n",
    "# Funci√≥n para obtener el embedding de un texto usando BERT.\n",
    "# Se utiliza el promedio de la √∫ltima capa (puedes ajustar esta estrategia si lo deseas)\n",
    "def obtener_embedding(texto):\n",
    "    inputs = tokenizer(texto, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Promediamos los embeddings de la secuencia (alternativamente se podr√≠a usar el token [CLS])\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Construir la lista de entradas de candidatos a partir de las dos columnas:\n",
    "# Se generan entradas separadas para cada candidato, indicando si es presidente o vice,\n",
    "# y se asocia el correspondiente valor de \"Whois\"\n",
    "candidatos = []\n",
    "for idx, row in df.iterrows():\n",
    "    # Si existe un candidato presidente, se agrega una entrada\n",
    "    if pd.notna(row['CandidatoPresidente']):\n",
    "        entry = {\n",
    "            'nombre': row['CandidatoPresidente'],\n",
    "            'tipo': 'presidente',\n",
    "            'whois': row['WhoisPresident']\n",
    "        }\n",
    "        candidatos.append(entry)\n",
    "    # Si existe un candidato vicepresidente, se agrega otra entrada\n",
    "    if pd.notna(row['CandidatoVicePresidente']):\n",
    "        entry = {\n",
    "            'nombre': row['CandidatoVicePresidente'],\n",
    "            'tipo': 'vice',\n",
    "            'whois': row['WhoisVice']\n",
    "        }\n",
    "        candidatos.append(entry)\n",
    "\n",
    "# Generar los embeddings para cada candidato (usando el nombre normalizado)\n",
    "embeddings = []\n",
    "for entry in candidatos:\n",
    "    nombre_normalizado = normalizar_texto(entry['nombre'])\n",
    "    emb = obtener_embedding(nombre_normalizado)\n",
    "    embeddings.append(emb)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Crear el √≠ndice FAISS utilizando la m√©trica L2 para b√∫squedas por similitud\n",
    "d = embeddings.shape[1]  # Dimensionalidad del embedding\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)\n",
    "\n",
    "def buscar_candidato_por_similitud(query_text):\n",
    "    # Convertir la consulta a min√∫sculas y remover palabras comunes (\"qui√©n\", \"quien\", \"es\")\n",
    "    query_lower = query_text.lower()\n",
    "    for palabra in [\"qui√©n\", \"quien\", \"es\"]:\n",
    "        query_lower = query_lower.replace(palabra, \"\")\n",
    "    candidate_query = query_lower.strip()\n",
    "    \n",
    "    # Normalizar el texto extra√≠do\n",
    "    normalized_candidate_query = normalizar_texto(candidate_query)\n",
    "    \n",
    "    # Obtener el embedding de la consulta\n",
    "    embedding_query = obtener_embedding(normalized_candidate_query)\n",
    "    embedding_query = np.array([embedding_query]).astype('float32')\n",
    "    \n",
    "    # Buscar el candidato m√°s cercano en el √≠ndice FAISS\n",
    "    D, I = index.search(embedding_query, 1)\n",
    "    \n",
    "    # Mostrar informaci√≥n de depuraci√≥n\n",
    "    print(\"Distancia obtenida:\", D[0][0])\n",
    "    print(\"√çndice en el √≠ndice FAISS:\", I[0][0])\n",
    "    \n",
    "    # Obtener el candidato encontrado\n",
    "    candidato_encontrado = candidatos[I[0][0]]\n",
    "    print(\"Candidato encontrado:\", candidato_encontrado)\n",
    "    return candidato_encontrado['whois']\n",
    "\n",
    "# Funci√≥n principal para procesar la consulta\n",
    "def hacer_consulta(query_text):\n",
    "    if \"qui√©n\" in query_text.lower() or \"quien\" in query_text.lower():\n",
    "        resultado = buscar_candidato_por_similitud(query_text)  # Llamada sin 'umbral'\n",
    "        print(\"Resultado:\", resultado)\n",
    "        return resultado\n",
    "    else:\n",
    "        print(\"Consulta no v√°lida.\")\n",
    "        return \"Consulta no v√°lida.\"\n",
    "\n",
    "# Ejemplo de consulta\n",
    "query_text = \"qui√©n es CARLOS \"\n",
    "print(\"\\n--- Consulta ---\")\n",
    "hacer_consulta(query_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genera y almacena embeddings de BERT para los nombres de candidatos (presidente y vicepresidente), permitiendo b√∫squedas sem√°nticas y comparaci√≥n de similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Embeddings de candidatos guardados en 'candidatos.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle\n",
    "import faiss\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv(\"candidatos.csv\", delimiter=\";\")\n",
    "\n",
    "# Inicializar el tokenizador y el modelo BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Funci√≥n para obtener embeddings de BERT\n",
    "def get_bert_embeddings(text):\n",
    "    if pd.isna(text):  # Manejar valores nulos en la columna\n",
    "        return np.zeros(768, dtype=np.float32)  # Devuelve un vector de ceros\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# Generar embeddings para presidente y vicepresidente\n",
    "df[\"presidente_embedding\"] = df[\"CandidatoPresidente\"].apply(get_bert_embeddings)\n",
    "df[\"vicepresidente_embedding\"] = df[\"CandidatoVicePresidente\"].apply(get_bert_embeddings)\n",
    "\n",
    "# Convertir los embeddings a una matriz NumPy para FAISS\n",
    "presidente_embeddings = np.vstack(df[\"presidente_embedding\"].values)\n",
    "vicepresidente_embeddings = np.vstack(df[\"vicepresidente_embedding\"].values)\n",
    "\n",
    "# Crear √≠ndice FAISS para presidente y vicepresidente\n",
    "index_presidente = faiss.IndexFlatL2(presidente_embeddings.shape[1])\n",
    "index_vicepresidente = faiss.IndexFlatL2(vicepresidente_embeddings.shape[1])\n",
    "\n",
    "index_presidente.add(presidente_embeddings.astype(\"float32\"))\n",
    "index_vicepresidente.add(vicepresidente_embeddings.astype(\"float32\"))\n",
    "\n",
    "# Guardar el √≠ndice FAISS y el DataFrame con nombres\n",
    "with open(\"candidatos.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"df_candidatos\": df,\n",
    "        \"index_presidente\": index_presidente,\n",
    "        \"index_vicepresidente\": index_vicepresidente\n",
    "    }, f)\n",
    "\n",
    "print(\"[INFO] Embeddings de candidatos guardados en 'candidatos.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\camia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\camia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cargando modelo BERT...\n",
      "[INFO] Cargando datos...\n",
      "\n",
      "‚úÖ Simulando consulta sin backend:\n",
      "[INFO] Candidato detectado: daniel noboa azin\n",
      "[INFO] Usando √≠ndice de candidatos.\n",
      "- Propuesta de daniel noboa azin sobre proponer noboa educaci√≥n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import faiss\n",
    "import pickle\n",
    "import re\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Descargar datos de NLP si no est√°n disponibles\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Cargar el modelo spaCy para espa√±ol\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Cargar el tokenizador y modelo BERT\n",
    "print(\"[INFO] Cargando modelo BERT...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ---------------------------\n",
    "# 1Ô∏è‚É£ Cargar los datos\n",
    "# ---------------------------\n",
    "print(\"[INFO] Cargando datos...\")\n",
    "\n",
    "# Cargar embeddings de oraciones\n",
    "with open(\"oraciones_con_embeddings_completo.pkl\", \"rb\") as f:\n",
    "    df1 = pickle.load(f)\n",
    "\n",
    "with open(\"./data/interview/elecciones_oraciones_con_embeddings.pkl\", \"rb\") as f:\n",
    "    df2 = pickle.load(f)\n",
    "\n",
    "# Cargar candidatos y embeddings\n",
    "with open(\"candidatos.pkl\", \"rb\") as f:\n",
    "    candidatos_data = pickle.load(f)\n",
    "\n",
    "df_candidatos = candidatos_data[\"df_candidatos\"]\n",
    "index_presidente = candidatos_data[\"index_presidente\"]\n",
    "index_vicepresidente = candidatos_data[\"index_vicepresidente\"]\n",
    "\n",
    "# ---------------------------\n",
    "# 2Ô∏è‚É£ Normalizar nombres de candidatos\n",
    "# ---------------------------\n",
    "def normalizar_texto(texto):\n",
    "    return re.sub(r\"[^a-z√°√©√≠√≥√∫√º√± ]\", \"\", texto.lower().strip())\n",
    "\n",
    "df_candidatos[\"CandidatoPresidente\"] = df_candidatos[\"CandidatoPresidente\"].apply(normalizar_texto)\n",
    "df_candidatos[\"CandidatoVicePresidente\"] = df_candidatos[\"CandidatoVicePresidente\"].apply(normalizar_texto)\n",
    "\n",
    "# Crear lista de nombres y diccionario con nombres simplificados\n",
    "lista_candidatos = df_candidatos[\"CandidatoPresidente\"].tolist() + df_candidatos[\"CandidatoVicePresidente\"].tolist()\n",
    "mapa_candidatos = {nombre: row[\"CandidatoPresidente\"] for _, row in df_candidatos.iterrows() for nombre in row[\"CandidatoPresidente\"].split()}\n",
    "\n",
    "# ---------------------------\n",
    "# 3Ô∏è‚É£ Funci√≥n de Preprocesamiento\n",
    "# ---------------------------\n",
    "def preprocess_query(query):\n",
    "    tokens = word_tokenize(query.lower(), language=\"spanish\")\n",
    "    stop_words = set(stopwords.words(\"spanish\"))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    cleaned_query = \" \".join(lemmatized_tokens)\n",
    "    return cleaned_query\n",
    "\n",
    "# ---------------------------\n",
    "# 4Ô∏è‚É£ Detecci√≥n de Candidato Mejorada\n",
    "# ---------------------------\n",
    "def detectar_candidato(query):\n",
    "    query_norm = normalizar_texto(query)\n",
    "    \n",
    "    # Intentar coincidencia exacta o parcial con los nombres\n",
    "    for nombre in query_norm.split():\n",
    "        if nombre in mapa_candidatos:\n",
    "            return \"presidente\", mapa_candidatos[nombre]\n",
    "\n",
    "    # Usar b√∫squeda difusa como respaldo\n",
    "    candidato_coincidencia = get_close_matches(query_norm, lista_candidatos, n=1, cutoff=0.5)\n",
    "\n",
    "    if candidato_coincidencia:\n",
    "        for _, row in df_candidatos.iterrows():\n",
    "            if row[\"CandidatoPresidente\"] == candidato_coincidencia[0]:\n",
    "                return \"presidente\", row[\"CandidatoPresidente\"]\n",
    "            elif row[\"CandidatoVicePresidente\"] == candidato_coincidencia[0]:\n",
    "                return \"vicepresidente\", row[\"CandidatoVicePresidente\"]\n",
    "\n",
    "    return None, None  # Si no se menciona ning√∫n candidato, retorna None\n",
    "\n",
    "# ---------------------------\n",
    "# 5Ô∏è‚É£ Obtener Embeddings con BERT\n",
    "# ---------------------------\n",
    "def obtener_embedding(texto):\n",
    "    inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "\n",
    "# ---------------------------\n",
    "# 6Ô∏è‚É£ Funci√≥n de b√∫squeda en FAISS con switch din√°mico\n",
    "# ---------------------------\n",
    "def query_faiss_simulacion(query_text, k=50, n_top=50):\n",
    "    tipo_candidato, candidato_detectado = detectar_candidato(query_text)\n",
    "    print(f\"[INFO] Candidato detectado: {candidato_detectado}\")\n",
    "\n",
    "    cleaned_query = preprocess_query(query_text)\n",
    "    query_embedding = obtener_embedding(cleaned_query).astype(\"float32\")\n",
    "    query_vector = np.expand_dims(query_embedding, axis=0)\n",
    "\n",
    "    resultados_con_dist = []\n",
    "\n",
    "    # **Switch para elegir el √≠ndice correcto**\n",
    "    if candidato_detectado:\n",
    "        print(\"[INFO] Usando √≠ndice de candidatos.\")\n",
    "        index = index_presidente if tipo_candidato == \"presidente\" else index_vicepresidente\n",
    "    else:\n",
    "        print(\"[INFO] Usando √≠ndices de oraciones generales.\")\n",
    "        if \"econom√≠a\" in cleaned_query or \"dinero\" in cleaned_query:\n",
    "            index = faiss.IndexFlatL2(query_embedding.shape[0])  # Simulaci√≥n\n",
    "        elif \"educaci√≥n\" in cleaned_query or \"escuela\" in cleaned_query:\n",
    "            index = faiss.IndexFlatL2(query_embedding.shape[0])  # Simulaci√≥n\n",
    "        else:\n",
    "            index = faiss.IndexFlatL2(query_embedding.shape[0])  # Simulaci√≥n\n",
    "\n",
    "    # **Simulaci√≥n de b√∫squeda en FAISS**\n",
    "    resultados_con_dist.append((0.1, f\"- Propuesta de {candidato_detectado if candidato_detectado else 'un candidato'} sobre {cleaned_query}\"))\n",
    "\n",
    "    # Ordenar y obtener top resultados\n",
    "    resultados_con_dist.sort(key=lambda x: x[0])\n",
    "    resultados_finales = [resultado for _, resultado in resultados_con_dist[:n_top]]\n",
    "\n",
    "    return \"\\n\".join(resultados_finales)\n",
    "\n",
    "# ---------------------------\n",
    "# 7Ô∏è‚É£ PRUEBA DE CONSULTA DIRECTA EN EL NOTEBOOK\n",
    "# ---------------------------\n",
    "consulta = \"Qu√© propone Noboa sobre la educaci√≥n?\"\n",
    "print(\"\\n‚úÖ Simulando consulta sin backend:\")\n",
    "respuesta_simulada = query_faiss_simulacion(consulta)\n",
    "print(respuesta_simulada)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\camia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\camia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cargando modelo BERT...\n",
      "[INFO] Cargando datos...\n",
      "\n",
      "‚úÖ Simulando consulta sin backend:\n",
      "[INFO] Candidato detectado: daniel noboa azin\n",
      "[INFO] Usando √≠ndice de candidatos.\n",
      "\n",
      "[INFO] Prompt enviado a Ollama:\n",
      "Consulta: Qu√© propone Daniel Noboa sobre la educaci√≥n?\n",
      "Estas son las propuestas m√°s relevantes encontradas:\n",
      "\n",
      "\n",
      "\n",
      "Genera un resumen claro y estructurado basado en estos datos.\n",
      "**Resumen de las Propuestas de Daniel Noboa sobre la Educaci√≥n**\n",
      "\n",
      "Daniel Noboa, expresidente del Ecuador, ha presentado varias propuestas para mejorar la educaci√≥n en el pa√≠s. A continuaci√≥n, se presentan las propuestas m√°s relevantes:\n",
      "\n",
      "*   **Aumento de la inversi√≥n en educaci√≥n**: Noboa ha propuesto aumentar la inversi√≥n en educaci√≥n para asegurar que los recursos sean suficientes para mejorar la calidad de la educaci√≥n.\n",
      "*   **Reforma del sistema educativo**: Noboa ha planteado la necesidad de reformar el sistema educativo para que sea m√°s eficiente y efectivo, lo que incluye la creaci√≥n de un sistema de evaluaci√≥n m√°s transparente.\n",
      "*   **Importancia de la educaci√≥n en el desarrollo sostenible**: Noboa ha subrayado la importancia de la educaci√≥n en el desarrollo sostenible, ya que es clave para promover la conciencia y el cambio social.\n",
      "\n",
      "En resumen, las propuestas de Daniel Noboa sobre la educaci√≥n enfocan en mejorar la calidad y accesibilidad del sistema educativo, as√≠ como en fomentar un enfoque m√°s sostenible y eficiente en el desarrollo.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import faiss\n",
    "import pickle\n",
    "import re\n",
    "import ollama\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Descargar datos de NLP si no est√°n disponibles\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Cargar el modelo spaCy para espa√±ol\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Cargar el tokenizador y modelo BERT\n",
    "print(\"[INFO] Cargando modelo BERT...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ---------------------------\n",
    "# 1Ô∏è‚É£ Cargar los datos\n",
    "# ---------------------------\n",
    "print(\"[INFO] Cargando datos...\")\n",
    "\n",
    "# Cargar embeddings de oraciones\n",
    "with open(\"oraciones_con_embeddings_completo.pkl\", \"rb\") as f:\n",
    "    df1 = pickle.load(f)\n",
    "\n",
    "with open(\"./data/interview/elecciones_oraciones_con_embeddings.pkl\", \"rb\") as f:\n",
    "    df2 = pickle.load(f)\n",
    "\n",
    "# Cargar candidatos y embeddings\n",
    "with open(\"candidatos.pkl\", \"rb\") as f:\n",
    "    candidatos_data = pickle.load(f)\n",
    "\n",
    "df_candidatos = candidatos_data[\"df_candidatos\"]\n",
    "index_presidente = candidatos_data[\"index_presidente\"]\n",
    "index_vicepresidente = candidatos_data[\"index_vicepresidente\"]\n",
    "\n",
    "# Asegurar que los nombres de las columnas est√°n limpios\n",
    "df_candidatos.columns = df_candidatos.columns.str.strip()\n",
    "\n",
    "# ---------------------------\n",
    "# 2Ô∏è‚É£ Normalizar nombres de candidatos\n",
    "# ---------------------------\n",
    "def normalizar_texto(texto):\n",
    "    return re.sub(r\"[^a-z√°√©√≠√≥√∫√º√± ]\", \"\", texto.lower().strip())\n",
    "\n",
    "df_candidatos[\"CandidatoPresidente\"] = df_candidatos[\"CandidatoPresidente\"].apply(normalizar_texto)\n",
    "df_candidatos[\"CandidatoVicePresidente\"] = df_candidatos[\"CandidatoVicePresidente\"].apply(normalizar_texto)\n",
    "\n",
    "# Crear lista de nombres y diccionario con nombres simplificados\n",
    "lista_candidatos = df_candidatos[\"CandidatoPresidente\"].tolist() + df_candidatos[\"CandidatoVicePresidente\"].tolist()\n",
    "mapa_candidatos = {nombre: row[\"CandidatoPresidente\"] for _, row in df_candidatos.iterrows() for nombre in row[\"CandidatoPresidente\"].split()}\n",
    "\n",
    "# ---------------------------\n",
    "# 3Ô∏è‚É£ Funci√≥n de Preprocesamiento\n",
    "# ---------------------------\n",
    "def preprocess_query(query):\n",
    "    tokens = word_tokenize(query.lower(), language=\"spanish\")\n",
    "    stop_words = set(stopwords.words(\"spanish\"))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    cleaned_query = \" \".join(lemmatized_tokens)\n",
    "    return cleaned_query\n",
    "\n",
    "# ---------------------------\n",
    "# 4Ô∏è‚É£ Detecci√≥n de Candidato\n",
    "# ---------------------------\n",
    "def detectar_candidato(query):\n",
    "    query_norm = normalizar_texto(query)\n",
    "\n",
    "    # Intentar coincidencia exacta o parcial con los nombres\n",
    "    for nombre in query_norm.split():\n",
    "        if nombre in mapa_candidatos:\n",
    "            return \"presidente\", mapa_candidatos[nombre]\n",
    "\n",
    "    # Usar b√∫squeda difusa como respaldo\n",
    "    candidato_coincidencia = get_close_matches(query_norm, lista_candidatos, n=1, cutoff=0.5)\n",
    "\n",
    "    if candidato_coincidencia:\n",
    "        for _, row in df_candidatos.iterrows():\n",
    "            if row[\"CandidatoPresidente\"] == candidato_coincidencia[0]:\n",
    "                return \"presidente\", row[\"CandidatoPresidente\"]\n",
    "            elif row[\"CandidatoVicePresidente\"] == candidato_coincidencia[0]:\n",
    "                return \"vicepresidente\", row[\"CandidatoVicePresidente\"]\n",
    "\n",
    "    return None, None  # Si no se menciona ning√∫n candidato, retorna None\n",
    "\n",
    "# ---------------------------\n",
    "# 5Ô∏è‚É£ Obtener Embeddings con BERT\n",
    "# ---------------------------\n",
    "def obtener_embedding(texto):\n",
    "    inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "\n",
    "# ---------------------------\n",
    "# 6Ô∏è‚É£ Funci√≥n de b√∫squeda en FAISS con Switch Din√°mico\n",
    "# ---------------------------\n",
    "def query_faiss_y_ollama(query_text, k=50, n_top=50):\n",
    "    tipo_candidato, candidato_detectado = detectar_candidato(query_text)\n",
    "    print(f\"[INFO] Candidato detectado: {candidato_detectado}\")\n",
    "\n",
    "    cleaned_query = preprocess_query(query_text)\n",
    "    query_embedding = obtener_embedding(cleaned_query).astype(\"float32\")\n",
    "    query_vector = np.expand_dims(query_embedding, axis=0)\n",
    "\n",
    "    resultados_con_dist = []\n",
    "\n",
    "    # **Switch para elegir el √≠ndice correcto**\n",
    "    if candidato_detectado:\n",
    "        print(\"[INFO] Usando √≠ndice de candidatos.\")\n",
    "        index = index_presidente if tipo_candidato == \"presidente\" else index_vicepresidente\n",
    "\n",
    "        distances, indices = index.search(query_vector, k)\n",
    "        for idx, dist in zip(indices[0], distances[0]):\n",
    "            fila = df1.iloc[idx]\n",
    "            if fila[\"CandidatoPresidente\"] == candidato_detectado:\n",
    "                resultado = f\"- {fila['Oracion']} (Partido: {fila['Partido']})\"\n",
    "                resultados_con_dist.append((dist, resultado))\n",
    "    else:\n",
    "        print(\"[INFO] Usando √≠ndices de oraciones generales.\")\n",
    "        distances, indices = index1.search(query_vector, k)\n",
    "\n",
    "        for idx, dist in zip(indices[0], distances[0]):\n",
    "            fila = df1.iloc[idx]\n",
    "            resultado = f\"- {fila['Oracion']} (Partido: {fila['Partido']})\"\n",
    "            resultados_con_dist.append((dist, resultado))\n",
    "\n",
    "    # Ordenar y obtener top resultados\n",
    "    resultados_con_dist.sort(key=lambda x: x[0])\n",
    "    resultados_finales = [resultado for _, resultado in resultados_con_dist[:n_top]]\n",
    "\n",
    "    # Construir el prompt para Ollama\n",
    "    prompt = (\n",
    "        f\"Consulta: {query_text}\\n\"\n",
    "        \"Estas son las propuestas m√°s relevantes encontradas:\\n\\n\"\n",
    "        + \"\\n\".join(resultados_finales)\n",
    "        + \"\\n\\nGenera un resumen claro y estructurado basado en estos datos.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n[INFO] Prompt enviado a Ollama:\")\n",
    "    print(prompt)\n",
    "\n",
    "    # Generar la respuesta con Ollama\n",
    "    respuesta = ollama.chat(model=\"llama3.2:latest\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    return respuesta[\"message\"][\"content\"]\n",
    "\n",
    "# ---------------------------\n",
    "# 7Ô∏è‚É£ PRUEBA DE CONSULTA DIRECTA EN EL NOTEBOOK\n",
    "# ---------------------------\n",
    "consulta = \"Qu√© propone Daniel Noboa sobre la educaci√≥n?\"\n",
    "print(\"\\n‚úÖ Simulando consulta sin backend:\")\n",
    "respuesta_simulada = query_faiss_y_ollama(consulta)\n",
    "print(respuesta_simulada)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexa embeddings de candidatos en FAISS y permite b√∫squedas sem√°nticas en lenguaje natural usando BERT, diferenciando entre presidente y vicepresidente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consultando: QUI√âN ES LUISA?\n",
      "Nombre extra√≠do para b√∫squeda: ES LUISA\n",
      "Resultado: Candidato Presidente: ANDREA GONZALEZ (Distancia: 54.80084228515625)\n",
      "Quien es: Andrea Gonz√°lez Nader es una activista medioambiental y pol√≠tica ecuatoriana de 37 a√±os, originaria de Guayaquil. Ha trabajado como profesora en instituciones educativas como los colegios Delta-Torremar, Cruz del Sur y la Unidad Educativa Biling√ºe Delta entre 2007 y 2015. Adem√°s, ha colaborado con organizaciones medioambientales como la Fundaci√≥n La Iguana y ha liderado proyectos de paisajismo sostenible y manejo integral de desechos. En 2023, fue candidata a la vicepresidencia de la Rep√∫blica junto al periodista Fernando Villavicencio, y posteriormente acompa√±√≥ al periodista Christian Zurita en el mismo proceso electoral. En agosto de 2024, fue proclamada candidata presidencial por el Partido Sociedad Patri√≥tica (PSP) para las elecciones de 2025.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re  # Para usar expresiones regulares y limpiar la consulta\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Cargar los datos desde el CSV\n",
    "df = pd.read_csv(\"candidatos.csv\", delimiter=\";\")\n",
    "\n",
    "# Cargar el modelo BERT y el tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Funci√≥n para obtener los embeddings de BERT\n",
    "def get_bert_embeddings(texto):\n",
    "    # Tokenizar el texto\n",
    "    inputs = tokenizer(texto, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Obtener la salida del modelo BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Obtener el embedding correspondiente a la √∫ltima capa (pooling)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Funci√≥n para normalizar los embeddings\n",
    "def normalizar_embeddings(embeddings):\n",
    "    return embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Suponiendo que ya tienes las funciones para obtener los embeddings\n",
    "presidente_embeddings = np.array([get_bert_embeddings(nombre).flatten() for nombre in df[\"CandidatoPresidente\"]])\n",
    "vicepresidente_embeddings = np.array([get_bert_embeddings(nombre).flatten() for nombre in df[\"CandidatoVicePresidente\"]])\n",
    "\n",
    "# Normalizar los embeddings\n",
    "presidente_embeddings = normalizar_embeddings(presidente_embeddings)\n",
    "vicepresidente_embeddings = normalizar_embeddings(vicepresidente_embeddings)\n",
    "\n",
    "# Concatenar los embeddings de presidente y vicepresidente\n",
    "all_embeddings = np.concatenate([presidente_embeddings, vicepresidente_embeddings], axis=0)\n",
    "\n",
    "# Crear un √≠ndice FAISS\n",
    "dimension = all_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(all_embeddings.astype(np.float32))\n",
    "\n",
    "# Funci√≥n de b√∫squeda y verificaci√≥n de \"qui√©n\" + nombre\n",
    "def buscar_similares(nuevo_texto, k=1):  # Solo devolver el top 1\n",
    "    nuevo_texto = nuevo_texto.upper()  # Convertir la consulta a may√∫sculas\n",
    "    print(f\"Consultando: {nuevo_texto}\")\n",
    "    \n",
    "    # Comprobar si la consulta contiene \"qui√©n\" y un nombre\n",
    "    if \"QUI√âN\" in nuevo_texto:\n",
    "        # Limpiar la consulta para extraer solo el nombre del candidato\n",
    "        nombre = re.sub(r'[^A-Za-z\\s]', '', nuevo_texto.replace(\"QUI√âN\", \"\").strip())  # Remover caracteres no alfab√©ticos\n",
    "        nombre = nombre.strip()  # Eliminar espacios adicionales\n",
    "        print(f\"Nombre extra√≠do para b√∫squeda: {nombre}\")\n",
    "        \n",
    "        # Realizar b√∫squeda para presidente y vicepresidente\n",
    "        # Primero buscamos el presidente\n",
    "        distancias_presidente, indices_presidente = index.search(get_bert_embeddings(nombre).reshape(1, -1).astype(np.float32), k)\n",
    "        distancias_presidente, indices_presidente = zip(*sorted(zip(distancias_presidente[0], indices_presidente[0]), key=lambda x: x[0]))\n",
    "\n",
    "        # Si encontramos una coincidencia con el presidente, lo devolvemos\n",
    "        for i, (distancia, indice) in enumerate(zip(distancias_presidente, indices_presidente)):\n",
    "            if indice < len(presidente_embeddings):  # Si es presidente\n",
    "                candidato = df['CandidatoPresidente'].iloc[indice]\n",
    "                print(f\"Resultado: Candidato Presidente: {candidato} (Distancia: {distancia})\")\n",
    "                print(f\"Quien es: {df['WhoisPresident'].iloc[indice]}\")\n",
    "                return\n",
    "            else:\n",
    "                # Si no es presidente, lo buscamos en vicepresidente\n",
    "                indice_vice = indice - len(presidente_embeddings)\n",
    "                candidato = df['CandidatoVicePresidente'].iloc[indice_vice]\n",
    "                print(f\"Resultado: Candidato Vicepresidente: {candidato} (Distancia: {distancia})\")\n",
    "                print(f\"Quien es: {df['WhoisVice'].iloc[indice_vice]}\")\n",
    "                return\n",
    "    else:\n",
    "        print(\"No se encontr√≥ la palabra 'qui√©n' en la consulta.\")\n",
    "\n",
    "# Ejemplo de consulta con \"qui√©n\"\n",
    "consulta = \"Qui√©n es luisa?\"\n",
    "buscar_similares(consulta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Itera sobre los vectores en FAISS, reconstruy√©ndolos para verificar la integridad del √≠ndice y confirmar que los embeddings est√°n correctamente almacenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la cantidad total de elementos en el √≠ndice FAISS\n",
    "print(\"Total de elementos en el √≠ndice FAISS:\", index.ntotal)\n",
    "\n",
    "# Iterar sobre cada elemento en el √≠ndice y mostrar su contenido\n",
    "for i in range(index.ntotal):\n",
    "    # Reconstruir el vector almacenado en el √≠ndice para el √≠tem i\n",
    "    vector_reconstruido = index.reconstruct(i)\n",
    "    \n",
    "    # Mostrar el √≠ndice, la informaci√≥n del candidato y (opcionalmente) el vector\n",
    "    print(f\"\\n√çndice {i}:\")\n",
    "    print(\"Informaci√≥n del candidato:\", candidatos[i])\n",
    "    print(\"Embedding (vector):\", vector_reconstruido)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza BERT y FAISS para buscar informaci√≥n relevante en documentos pol√≠ticos, generando un resumen con Ollama basado en los resultados m√°s similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\camia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\camia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se agregaron 13759 vectores al √≠ndice FAISS.\n",
      "Se agregaron 7753 vectores al √≠ndice FAISS.\n",
      "\n",
      "--- Realizando consulta en ambos √≠ndices ---\n",
      "Consulta preprocesada: propuesta economio\n",
      "\n",
      "Respuesta de Ollama:\n",
      "Bas√°ndonos en los resultados proporcionados, se puede observar que hay varios partidos pol√≠ticos y candidatos que presentaron propuestas y estrategias para abordar diversos desaf√≠os nacionales, como la econom√≠a, el medio ambiente, la educaci√≥n, la salud, la seguridad y la protecci√≥n de los recursos naturales. A continuaci√≥n, se resumen algunos de los puntos clave:\n",
      "\n",
      "* La mayor√≠a de los partidos pol√≠ticos presentaron propuestas relacionadas con la econom√≠a, enfatizando la importancia de un modelo de desarrollo sostenible y circular.\n",
      "* Los partidos que m√°s se destacaron en este aspecto fueron el Partido Izquierda Democr√°tica (PID) y el Movimiento Constructivo (CON), quienes presentaron planes para potenciar la extracci√≥n hidrocarburifera de manera ecol√≥gica, fortalecer la educaci√≥n y formaci√≥n √©tico-tecnol√≥gica calidad, y implementar pol√≠ticas de econom√≠a circular y protecci√≥n de la biodiversidad.\n",
      "* En cuanto a la protecci√≥n del medio ambiente, los partidos presentaron propuestas para mitigar el efecto del cambio clim√°tico, como la reducci√≥n de emisiones de gases de efecto invernadero y la implementaci√≥n de pol√≠ticas para proteger la biodiversidad.\n",
      "* La salud y la educaci√≥n fueron otro tema clave en las propuestas de varios partidos, con un √©nfasis en la importancia de estos servicios p√∫blicos para el bienestar de la sociedad ecuatoriana.\n",
      "* En cuanto a la seguridad y la justicia, algunos partidos presentaron propuestas para fortalecer la seguridad ciudadana y garantizar la igualdad ante la ley.\n",
      "\n",
      "En general, se puede observar que los partidos pol√≠ticos que m√°s se destacaron en este aspecto fueron aquellos que presentaron propuestas con un √©nfasis en la sostenibilidad, la protecci√≥n del medio ambiente y la justicia social. Sin embargo, tambi√©n es importante destacar que cada partido tiene sus propias fortalezas y debilidades, y que la elecci√≥n de un candidato o partido depender√° de las prioridades y valores de los electores ecuatorianos.\n",
      "\n",
      "En cuanto a la ausencia de representaci√≥n de algunos temas, como la desigualdad econ√≥mica, la corrupci√≥n y la participaci√≥n ciudadana, se puede observar que estos temas fueron menos abordados en las propuestas presentadas por los partidos pol√≠ticos. Sin embargo, algunos partidos como el Partido Socialista Ecuatoriano (PSE) presentaron propuestas para abordar estas cuestiones.\n",
      "\n",
      "En resumen, los resultados permiten observar que la econom√≠a sostenible y circular es un tema clave en las propuestas de varios partidos pol√≠ticos, mientras que la protecci√≥n del medio ambiente, la salud y la educaci√≥n tambi√©n son temas importantes. Sin embargo, hay una ausencia de representaci√≥n de algunos temas clave como la desigualdad econ√≥mica, la corrupci√≥n y la participaci√≥n ciudadana.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import faiss\n",
    "\n",
    "# Para preprocesamiento\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Descargar recursos de NLTK (si a√∫n no se han descargado)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Cargar el modelo spaCy para espa√±ol (aseg√∫rate de tener instalado \"es_core_news_sm\")\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Cargar el tokenizador y el modelo BERT (para obtener embeddings)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def obtener_embedding(texto):\n",
    "    \"\"\"\n",
    "    Dada una cadena de texto, devuelve el embedding correspondiente usando BERT.\n",
    "    Se utiliza el embedding del token [CLS] como representaci√≥n.\n",
    "    \"\"\"\n",
    "    texto = str(texto)  # asegurar que sea string\n",
    "    inputs = tokenizer(texto, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Se toma el embedding del token [CLS] (posici√≥n 0) y se aplana a un vector 1D (768,)\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "    return embedding\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Cargar los DataFrames con embeddings desde los archivos .pkl\n",
    "# ---------------------------------------------------------------------------\n",
    "with open('oraciones_con_embeddings_completo.pkl', 'rb') as f:\n",
    "    df1 = pickle.load(f)\n",
    "    \n",
    "with open('./data/interview/elecciones_oraciones_con_embeddings.pkl', 'rb') as f:\n",
    "    df2 = pickle.load(f)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Reconstruir los √≠ndices FAISS a partir de los embeddings almacenados en los DataFrames\n",
    "# ---------------------------------------------------------------------------\n",
    "def construir_index_faiss(df, embedding_col):\n",
    "    \"\"\"\n",
    "    Dado un DataFrame y el nombre de la columna que contiene los embeddings,\n",
    "    crea y devuelve un √≠ndice FAISS usando IndexFlatL2.\n",
    "    \"\"\"\n",
    "    embeddings_matrix = np.vstack(df[embedding_col].values).astype('float32')\n",
    "    dim = embeddings_matrix.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings_matrix)\n",
    "    print(f\"Se agregaron {index.ntotal} vectores al √≠ndice FAISS.\")\n",
    "    return index\n",
    "\n",
    "# Se asume que en df1 la columna de embeddings se llama 'embedding_Oracion'\n",
    "index1 = construir_index_faiss(df1, 'embedding_Oracion')\n",
    "\n",
    "# Se asume que en df2 la columna de embeddings se llama 'embedding_oracion_sinStopWords'\n",
    "index2 = construir_index_faiss(df2, 'embedding_oracion_sinStopWords')\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Funci√≥n para preprocesar la consulta\n",
    "# ---------------------------------------------------------------------------\n",
    "def preprocess_query(query):\n",
    "    \"\"\"\n",
    "    Preprocesa la consulta:\n",
    "      - Pasa a min√∫sculas.\n",
    "      - Tokeniza el texto en espa√±ol.\n",
    "      - Elimina tokens que no sean alfab√©ticos y las stopwords en espa√±ol.\n",
    "      - Lematiza el texto usando spaCy.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(query.lower(), language='spanish')\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    cleaned_query = ' '.join(lemmatized_tokens)\n",
    "    print(f\"Consulta preprocesada: {cleaned_query}\")\n",
    "    return cleaned_query\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. Funci√≥n para hacer la consulta en ambos √≠ndices y usar Ollama para generar una respuesta\n",
    "# ---------------------------------------------------------------------------\n",
    "def query_faiss_ollama(query_text, k=5, n_top=5):\n",
    "    \"\"\"\n",
    "    Preprocesa la consulta, obtiene su embedding y la consulta en dos √≠ndices FAISS.\n",
    "    Combina los resultados, ordena por similitud (menor distancia) y toma solo los n_top m√°s similares.\n",
    "    Luego construye un prompt para enviar a Ollama.\n",
    "    \n",
    "    Par√°metros:\n",
    "      - query_text: texto de consulta.\n",
    "      - k: n√∫mero de vecinos a recuperar en cada √≠ndice.\n",
    "      - n_top: n√∫mero final de resultados (combinados de ambos √≠ndices) a utilizar.\n",
    "    \"\"\"\n",
    "    # Preprocesar la consulta\n",
    "    cleaned_query = preprocess_query(query_text)\n",
    "    \n",
    "    # Generar el embedding para la consulta (aseg√∫rate de que sea float32)\n",
    "    query_embedding = obtener_embedding(cleaned_query).astype('float32')\n",
    "    query_vector = np.expand_dims(query_embedding, axis=0)\n",
    "    \n",
    "    # Buscar en el primer √≠ndice (df1)\n",
    "    distances1, indices1 = index1.search(query_vector, k)\n",
    "    # Buscar en el segundo √≠ndice (df2)\n",
    "    distances2, indices2 = index2.search(query_vector, k)\n",
    "    \n",
    "    # Combinar resultados de ambos √≠ndices junto con sus distancias\n",
    "    resultados_con_dist = []\n",
    "    \n",
    "    # Procesar resultados del primer √≠ndice (df1)\n",
    "    for idx, dist in zip(indices1[0], distances1[0]):\n",
    "        fila = df1.iloc[idx]\n",
    "        # Se asume que df1 tiene una columna \"Oracion\" para la oraci√≥n original\n",
    "        resultado = (\n",
    "            f\"oracion_plan: {fila['Oracion']}, \"\n",
    "            f\"Temas clave: {fila['Temas Clave']}, \"\n",
    "            f\"Partido: {fila['Partido']}, \"\n",
    "            f\"Candidato Presidente: {fila['ListaPolitica']}, \"\n",
    "            f\"Candidato Vicepresidente: {fila['CandidatoPresidente']}\"\n",
    "        )\n",
    "        resultados_con_dist.append((dist, resultado))\n",
    "    \n",
    "    # Procesar resultados del segundo √≠ndice (df2)\n",
    "    for idx, dist in zip(indices2[0], distances2[0]):\n",
    "        fila = df2.iloc[idx]\n",
    "        # Se asume que df2 tiene una columna \"oracion_original\" para la oraci√≥n original\n",
    "        resultado = (\n",
    "            f\"oracion_entrevista: {fila['oracion_original']}, \"\n",
    "            f\"Partido: {fila['Partido']}, \"\n",
    "            f\"Candidato Presidente: {fila['presidente']}, \"\n",
    "            f\"Candidato Vicepresidente: {fila['vicepresidente']}\"\n",
    "        )\n",
    "        resultados_con_dist.append((dist, resultado))\n",
    "    \n",
    "    # Ordenar los resultados combinados por distancia (la menor distancia indica mayor similitud)\n",
    "    resultados_con_dist.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Tomar solo los n_top resultados con mayor similitud\n",
    "    resultados_finales = [resultado for _, resultado in resultados_con_dist[:n_top]]\n",
    "    \n",
    "    # Construir el prompt para enviar a Ollama\n",
    "    prompt = (\n",
    "        \"Esto es lo que he encontrado.:\\n\\n\" +\n",
    "        \"\\n\".join(resultados_finales) +\n",
    "        \"\\n\\nGenera un resumen basado en estos resultados.\"\n",
    "    )\n",
    "    \n",
    "    # Llamar a Ollama (aseg√∫rate de tener la librer√≠a 'ollama' instalada y configurada)\n",
    "    import ollama\n",
    "    respuesta = ollama.chat(model='llama3.2:latest', messages=[{'role': 'user', 'content': prompt}])\n",
    "    return respuesta['message']['content']\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Ejemplo de consulta\n",
    "# ---------------------------------------------------------------------------\n",
    "query_text = \"propuestas sobre la economia?\"\n",
    "print(\"\\n--- Realizando consulta en ambos √≠ndices ---\")\n",
    "# En este ejemplo, se buscan 50 vecinos en cada √≠ndice, pero se usar√°n solo los 5 resultados\n",
    "respuesta_ollama = query_faiss_ollama(query_text, k=50, n_top=50)\n",
    "print(\"\\nRespuesta de Ollama:\")\n",
    "print(respuesta_ollama)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realiza b√∫squeda sem√°ntica con FAISS en planes de gobierno y discursos pol√≠ticos, generando un resumen con Ollama basado en los resultados m√°s relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\camia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\camia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto preprocesado: bajar el iva para fortalecer la economia\n",
      "Consulta procesada: bajar ivo fortalecer economia\n",
      "El resumen de las declaraciones se centra en los principales temas y propuestas presentadas por los partidos pol√≠ticos y l√≠deres mencionados. A continuaci√≥n, se detallan algunos de los puntos clave:\n",
      "\n",
      "**Desarrollo y Econom√≠a**\n",
      "\n",
      "* Se enfatiza la importancia del desarrollo sostenible y eficiente, especialmente en relaci√≥n con la lucha contra la malnutrici√≥n.\n",
      "* Se propone implementar estrategias intersectoriales para fomentar el crecimiento econ√≥mico y mejorar las condiciones de vida de la poblaci√≥n.\n",
      "\n",
      "**Justicia y Seguridad**\n",
      "\n",
      "* Se aboga por fortalecer el sistema de justicia, especialmente en relaci√≥n con la aplicaci√≥n de la ley y la lucha contra el crimen organizado.\n",
      "* Se considera fundamental aumentar la seguridad y proteger a los ciudadanos, especialmente en regiones remotas y aisladas.\n",
      "\n",
      "**Educaci√≥n**\n",
      "\n",
      "* Se destaca la importancia de la educaci√≥n superior y se busca hacer que sea m√°s accesible para todos.\n",
      "* Se propone implementar programas que promuevan el empoderamiento de las mujeres y les ofrezcan oportunidades de formaci√≥n y empleo.\n",
      "\n",
      "**Salud**\n",
      "\n",
      "* Se enfatiza la necesidad de mejorar la calidad del sistema de salud p√∫blico y privado, especialmente en relaci√≥n con la recertificaci√≥n peri√≥dica y la integraci√≥n de servicios.\n",
      "* Se considera fundamental priorizar el desarrollo estrat√©gico para abordar problemas como la malnutrici√≥n.\n",
      "\n",
      "**Tecnolog√≠a y Justicia**\n",
      "\n",
      "* Se propone implementar tecnolog√≠as avanzadas para mejorar la seguridad, como sistemas de reconocimiento facial y control constante a trav√©s de CCTV.\n",
      "* Se busca fortalecer el valor c√≠vico en Ecuador y crear una sociedad m√°s resiliente y menos vulnerable a la influencia del crimen organizado.\n",
      "\n",
      "En general, las declaraciones destacan la importancia de abordar problemas como la pobreza, la desigualdad y la inseguridad, y proponen estrategias para promover el desarrollo sostenible y la justicia social.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import nltk\n",
    "import spacy\n",
    "import ollama  # Importar la librer√≠a para usar Ollama\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Verificar si hay GPU disponible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Cargar el archivo CSV con las oraciones procesadas\n",
    "df = pd.read_csv('oraciones_procesadas_completo.csv', delimiter=';')\n",
    "\n",
    "# Cargar el archivo CSV con los datos de los partidos y candidatos\n",
    "candidatos_df = pd.read_csv('candidatos.csv', delimiter=';')  # Aseg√∫rate de que el archivo correcto est√© aqu√≠\n",
    "\n",
    "# Inicializar el modelo BERT preentrenado y moverlo a la GPU si est√° disponible\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Cargar el √≠ndice FAISS previamente guardado\n",
    "index = faiss.read_index('faiss_index.index')\n",
    "\n",
    "# Cargar los datos de oraciones\n",
    "with open('sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "# Cargar el modelo de spaCy para lematizaci√≥n en espa√±ol\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Funci√≥n para preprocesar el texto\n",
    "def preprocess_text(text):\n",
    "    print(f\"Texto preprocesado: {text}\")\n",
    "    tokens = word_tokenize(text.lower(), language='spanish')\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Funci√≥n para obtener la oraci√≥n original sin procesar y sus datos adicionales\n",
    "def get_oracion_data_by_id(oracion_id):\n",
    "    oracion_row = df[df['Oracion_ID'] == oracion_id]\n",
    "    if not oracion_row.empty:\n",
    "        oracion = oracion_row.iloc[0]['Oracion']\n",
    "        temas_clave = oracion_row.iloc[0]['Temas Clave']\n",
    "        return oracion, temas_clave\n",
    "    return None, None\n",
    "\n",
    "def get_partido_data_by_id(id):\n",
    "    candidato_row = candidatos_df[candidatos_df['ID'] == id]  # Buscamos usando el 'ID'\n",
    "    if not candidato_row.empty:\n",
    "        partido = candidato_row.iloc[0]['Partido']\n",
    "        candidato_presidente = candidato_row.iloc[0]['CandidatoPresidente']\n",
    "        candidato_vicepresidente = candidato_row.iloc[0]['CandidatoVicePresidente']\n",
    "        lista_politica = candidato_row.iloc[0]['ListaPolitica']\n",
    "        return partido, candidato_presidente, candidato_vicepresidente, lista_politica\n",
    "    return None, None, None, None\n",
    "\n",
    "def query_faiss_ollama(query, top_k=50):  \n",
    "    # Preprocesar la consulta\n",
    "    cleaned_query = preprocess_text(query)\n",
    "    # Mostrar el tama√±o de la consulta procesada\n",
    "    print(f\"Consulta procesada: {cleaned_query}\")\n",
    "    # Generar el embedding para la consulta\n",
    "    query_embedding = model.encode([cleaned_query], device=device).astype('float32')\n",
    "    # Realizar la b√∫squeda en el √≠ndice FAISS\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    # Procesar los resultados\n",
    "    resultados = []\n",
    "    for i in range(top_k):\n",
    "        # Obtener el ID de la oraci√≥n correspondiente\n",
    "        oracion_id = sentences[I[0][i]]['Oracion_ID']\n",
    "\n",
    "        id= sentences[I[0][i]]['ID']\n",
    "        # Buscar la oraci√≥n original y sus temas clave usando Oracion_ID\n",
    "        oracion_original, temas_clave = get_oracion_data_by_id(oracion_id)\n",
    "\n",
    "        # Buscar los datos del partido usando el ID del candidato\n",
    "        partido, candidato_presidente, candidato_vicepresidente, lista_politica = get_partido_data_by_id(id)\n",
    "\n",
    "        if oracion_original:  # Si se encuentra la oraci√≥n original\n",
    "            resultado = (\n",
    "                f\"Oraci√≥n: {oracion_original}, \"\n",
    "                f\"Temas Clave: {temas_clave}, \"\n",
    "                f\"Partido: {partido}, \"\n",
    "                f\"Presidente: {candidato_presidente}, \"\n",
    "                f\"Vicepresidente: {candidato_vicepresidente}, \"\n",
    "                f\"Lista Pol√≠tica: {lista_politica}\"\n",
    "            )\n",
    "            resultados.append(resultado)\n",
    "        \n",
    "    # Verificar si hemos obtenido resultados\n",
    "    if not resultados:\n",
    "        return \"No se encontraron oraciones relevantes para la consulta.\"\n",
    "\n",
    "    # Construir el prompt para Ollama con las oraciones obtenidas\n",
    "    prompt = f\"quiero que menciones los nombres de los Presidente,Vicepresidente,Lista Pol√≠tica y el partido de lo que he encontrado clasificalo:\\n\\n\" + \"\\n\".join(resultados) + \"\\n\\nGenera un resumen basado en estas declaraciones, expl√≠calo.\"\n",
    "\n",
    "    # Generar la respuesta con Ollama\n",
    "    respuesta = ollama.chat(model='llama3.2:latest', messages=[{'role': 'user', 'content': prompt}])\n",
    "\n",
    "    return respuesta['message']['content']\n",
    "\n",
    "# Ejemplo de consulta para verificar el funcionamiento\n",
    "query = \"bajar el iva para fortalecer la economia\"\n",
    "respuesta = query_faiss_ollama(query)\n",
    "print(respuesta)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
